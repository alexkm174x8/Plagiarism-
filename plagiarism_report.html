<html>
<head>
<title>Plagiarism Detector</title>
<style>

@font-face {
   font-family: Nohemi;
   src: url("Nohemi-Medium.otf")
}

@font-face {
   font-family: OffBit;
   src: url("OffBit.ttf")
}

body {
   background-color: #ffa1d7;
   font-family: Nohemi;
   color: #d3163b;
   margin: 0;
   padding: 0;
   box-sizing: border-box;
}

.container {
   max-width: 1370px;
   margin: 20px auto;
   padding: 20px;
   border: 1px solid #d3163b;
   background-color: #fdf2f5;
   box-sizing: border-box;
}

pre {
   border: 1px solid #d3163b;
   padding: 10px;
   background-color: #f8f7f8; 
   width: 100%;
   white-space: pre-wrap;
   word-wrap: break-word;
   box-sizing: border-box;
}

h1 {
   font-size: 60px;
   text-align: center;
   margin-top: 50px;
   font-family: OffBit;
}

h2 {
   margin-top: 5px;
   margin-bottom: 5px;
   font-size: 20px;
}

h3 {
   margin-top: 0px;
   font-size: 22px;
   margin-bottom: 45px;
   margin-left: 60px;
   margin-right: 60px;
}

description {
   margin-top: 0px;
   font-size: 25px;
   margin-bottom: 45px;
   font-family: OffBit;
}

.similarity-box {
   margin-bottom: 20px;
}

.text-container {
   display: block; 
}

.highlight {
   background-color: yellow;
   color: #d3163b;
}
</style>
</head>
<body>
<h1>Plagiarism Detector •*.✸ </h1>
<h3 style="text-align: center;">Web App in Golang that compares the content of different texts about the same topic, calculating and highlighting their similarity using a Suffix Array that retrieves the Longest Common Substring (LCS).</h3>
        <div class="container">
            <description>1. orig_taskd.txt vs g3pA_taskd.txt</description>
            <h2>Similarity: 97.99%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">in probability theory, bayes' theorem (often called bayes' law after rev thomas bayes) relates the conditional and marginal probabilities of two random events. it is often used to compute posterior probabilities given observations</span>. <span style="background-color: yellow; color: #d3163b;">for example, a patient may be observed to have certain symptoms</span><span style="background-color: yellow; color: #d3163b;">. bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation.</span> (see example 2)<span style="background-color: yellow; color: #d3163b;"> as a formal theorem, bayes' theorem is valid in all common interpretations of probability. however, it plays a central role in the debate around the foundations of statistics</span>:<span style="background-color: yellow; color: #d3163b;"> frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole, while bayesians describe probabilities in terms of beliefs and degrees of uncertainty. the articles on bayesian probability and frequentist probability discuss these debates in greater detail. bayes' theorem relates the conditional and marginal probabilities of events a and b, where b has a non-vanishing probability: p(a|b) = </span>\frac{<span style="background-color: yellow; color: #d3163b;">p(b | a)</span>\,<span style="background-color: yellow; color: #d3163b;"> p(a)</span>}{<span style="background-color: yellow; color: #d3163b;">p(b)</span>}<span style="background-color: yellow; color: #d3163b;">. each term in bayes' theorem has a conventional name:</span> *<span style="background-color: yellow; color: #d3163b;"><span style="background-color: yellow; color: #d3163b;"> p(a)</span> is the prior probability or marginal probability of a. it is "prior" in the sense that it does not take into account any information about b.</span> *<span style="background-color: yellow; color: #d3163b;"> p(a|b) is the conditional probability of a, given b. it is also called the posterior probability because it is derived from or depends upon the specified value of b. </span>* <span style="background-color: yellow; color: #d3163b;">p(b|a) is the conditional probability of b given a.</span> *<span style="background-color: yellow; color: #d3163b;"> <span style="background-color: yellow; color: #d3163b;">p(b)</span> is the prior or marginal probability of b, and acts as a normalizing constant. intuitively, bayes' theorem in this form describes the way in which one's beliefs about observing 'a' are updated by having observed 'b'.</span></pre>
                </div>
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">in probability theory, bayes' theorem (often called bayes' law after rev thomas bayes) relates the conditional and marginal probabilities of two random events. it is often used to compute posterior probabilities given observations</span> (<span style="background-color: yellow; color: #d3163b;">for example, a patient may be observed to have certain symptoms</span>)<span style="background-color: yellow; color: #d3163b;">. bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation.</span><span style="background-color: yellow; color: #d3163b;"> as a formal theorem, bayes' theorem is valid in all common interpretations of probability. however, it plays a central role in the debate around the foundations of statistics</span>;<span style="background-color: yellow; color: #d3163b;"> frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole, while bayesians describe probabilities in terms of beliefs and degrees of uncertainty. the articles on bayesian probability and frequentist probability discuss these debates in greater detail. bayes' theorem relates the conditional and marginal probabilities of events a and b, where b has a non-vanishing probability: p(a|b) = </span>(<span style="background-color: yellow; color: #d3163b;">p(b | a)</span> x<span style="background-color: yellow; color: #d3163b;"> p(a)</span>) / <span style="background-color: yellow; color: #d3163b;">p(b)</span><span style="background-color: yellow; color: #d3163b;">. each term in bayes' theorem has a conventional name:</span><span style="background-color: yellow; color: #d3163b;"><span style="background-color: yellow; color: #d3163b;"> p(a)</span> is the prior probability or marginal probability of a. it is "prior" in the sense that it does not take into account any information about b.</span><span style="background-color: yellow; color: #d3163b;"> p(a|b) is the conditional probability of a, given b. it is also called the posterior probability because it is derived from or depends upon the specified value of b. </span><span style="background-color: yellow; color: #d3163b;">p(b|a) is the conditional probability of b given a.</span><span style="background-color: yellow; color: #d3163b;"> <span style="background-color: yellow; color: #d3163b;">p(b)</span> is the prior or marginal probability of b, and acts as a normalizing constant. intuitively, bayes' theorem in this form describes the way in which one's beliefs about observing 'a' are updated by having observed 'b'.</span></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>2. g4pC_taskd.txt vs g3pA_taskd.txt</description>
            <h2>Similarity: 94.36%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">in probability theory, bayes' theorem</span><span style="background-color: yellow; color: #d3163b;"> relates the conditional and marginal probabilities of two random events. it is </span>usually be<span style="background-color: yellow; color: #d3163b;"> used to compute posterior probabilities given observations</span>. <span style="background-color: yellow; color: #d3163b;">for </span>instanc<span style="background-color: yellow; color: #d3163b;">e, a patient may be observed to have certain symptoms</span><span style="background-color: yellow; color: #d3163b;">. bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation. as a formal theorem, bayes' theorem is valid in all common interpretations of probability. however, it plays a central role in the debate around the foundations of statistics</span>:<span style="background-color: yellow; color: #d3163b;"> frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications</span><span style="background-color: yellow; color: #d3163b;">. the articles on bayesian probability and frequentist probability discuss these debates in greater detail</span><span style="background-color: yellow; color: #d3163b;">. frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole</span>. at the same time,<span style="background-color: yellow; color: #d3163b;"> bayesians describe probabilities in terms of beliefs and degrees of uncertainty</span><span style="background-color: yellow; color: #d3163b;">. bayes' theorem relates the conditional and marginal probabilities of events a and b, where b has a non-vanishing probability: </span><span style="background-color: yellow; color: #d3163b;">each term in bayes' theorem has a conventional name:</span> •<span style="background-color: yellow; color: #d3163b;"> p(a) is the prior probability or marginal probability of a. it is "prior" in the sense that it does not take into account any information about b.</span> •<span style="background-color: yellow; color: #d3163b;"> p(a|b) is the conditional probability of a, given b. it is also called the posterior probability because it is derived from or depends upon the specified value of b. </span>•<span style="background-color: yellow; color: #d3163b;"> p(b|a) is the conditional probability of b given a.</span> •<span style="background-color: yellow; color: #d3163b;"> p(b) is the prior or marginal probability of b, and acts as a normalizing constant. intuitively, bayes' theorem in this form describes the way in which one's beliefs about observing 'a' are updated by having observed 'b'.</span></pre>
                </div>
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">in probability theory, bayes' theorem</span> (often called bayes' law after rev thomas bayes)<span style="background-color: yellow; color: #d3163b;"> relates the conditional and marginal probabilities of two random events. it is </span>often<span style="background-color: yellow; color: #d3163b;"> used to compute posterior probabilities given observations</span> (<span style="background-color: yellow; color: #d3163b;">for </span>exampl<span style="background-color: yellow; color: #d3163b;">e, a patient may be observed to have certain symptoms</span>)<span style="background-color: yellow; color: #d3163b;">. bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation. as a formal theorem, bayes' theorem is valid in all common interpretations of probability. however, it plays a central role in the debate around the foundations of statistics</span>;<span style="background-color: yellow; color: #d3163b;"> frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications</span><span style="background-color: yellow; color: #d3163b;">. frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole</span>, while<span style="background-color: yellow; color: #d3163b;"> bayesians describe probabilities in terms of beliefs and degrees of uncertainty</span><span style="background-color: yellow; color: #d3163b;">. the articles on bayesian probability and frequentist probability discuss these debates in greater detail</span><span style="background-color: yellow; color: #d3163b;">. bayes' theorem relates the conditional and marginal probabilities of events a and b, where b has a non-vanishing probability: </span>p(a|b) = (p(b | a) x p(a)) / p(b). <span style="background-color: yellow; color: #d3163b;">each term in bayes' theorem has a conventional name:</span><span style="background-color: yellow; color: #d3163b;"> p(a) is the prior probability or marginal probability of a. it is "prior" in the sense that it does not take into account any information about b.</span><span style="background-color: yellow; color: #d3163b;"> p(a|b) is the conditional probability of a, given b. it is also called the posterior probability because it is derived from or depends upon the specified value of b. </span>p(b|a) is the conditional probability of b given a.<span style="background-color: yellow; color: #d3163b;"> p(b) is the prior or marginal probability of b, and acts as a normalizing constant. intuitively, bayes' theorem in this form describes the way in which one's beliefs about observing 'a' are updated by having observed 'b'.</span></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>3. g0pE_taska.txt vs g4pC_taska.txt</description>
            <h2>Similarity: 94.33%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">in object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. the inheritance concept was invented in 1967 for simula</span><span style="background-color: yellow; color: #d3163b;">. the new classes, known as derived classes, take over (or inherit) attribute</span><span style="background-color: yellow; color: #d3163b;"> and behavio</span>u<span style="background-color: yellow; color: #d3163b;">r of the pre-existing classes, which are referre<span style="background-color: yellow; color: #d3163b;">d to </span>as base classes (or ancestor classes). it is intende<span style="background-color: yellow; color: #d3163b;">d to </span>help reuse existing code with little or no modification</span>.<span style="background-color: yellow; color: #d3163b;"> inheritance provides the support for representation by categorization in computer languages. categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization </span>(w<span style="background-color: yellow; color: #d3163b;">hat is </span>known about specific entities is applie<span style="background-color: yellow; color: #d3163b;">d to </span>a wider group give<span style="background-color: yellow; color: #d3163b;">n a </span>belongs relation can be established) <span style="background-color: yellow; color: #d3163b;">and cognitive economy (less information needs to be stored about each specific entity, only its particularities)</span><span style="background-color: yellow; color: #d3163b;">. inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. for instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. one can consider fruit to be an abstraction of apple, orange, etc. conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant. an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code. inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.</span></pre>
                </div>
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">in object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. the inheritance concept was invented in 1967 for simula</span><span style="background-color: yellow; color: #d3163b;"> inheritance provides the support for representation by categorization in computer languages. categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization </span><span style="background-color: yellow; color: #d3163b;">and cognitive economy (less information needs to be stored about each specific entity, only its particularities)</span><span style="background-color: yellow; color: #d3163b;">. the new classes, known as derived classes, take over (or inherit) attribute</span>s<span style="background-color: yellow; color: #d3163b;"> and behavio</span><span style="background-color: yellow; color: #d3163b;">r of the pre-existing classes, which are referre<span style="background-color: yellow; color: #d3163b;">d to </span>as base classes (or ancestor classes). it is intende<span style="background-color: yellow; color: #d3163b;">d to </span>help reuse existing code with little or no modification</span><span style="background-color: yellow; color: #d3163b;">. inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. for instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. one can consider fruit to be an abstraction of apple, orange, etc. conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant. an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code. inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.</span> complex inheritance, or inheritance used withi<span style="background-color: yellow; color: #d3163b;">n a </span>design t<span style="background-color: yellow; color: #d3163b;">hat is </span>not sufficiently mature, may lea<span style="background-color: yellow; color: #d3163b;">d to </span>the yo-yo problem.</pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>4. orig_taska.txt vs g4pC_taska.txt</description>
            <h2>Similarity: 94.30%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">in object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. the inheritance concept was invented in 1967 for simula</span><span style="background-color: yellow; color: #d3163b;">. the new classes, known as derived classes, take over (or inherit) attributes and behavior of the pre-existing classes, which are referred to as base classes (or ancestor classes). it is intended to help reuse existing code with little or no modification</span>.<span style="background-color: yellow; color: #d3163b;"> inheritance provides the support for representation by categorization in computer languages. categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization </span>(what is known about specific entities is applied to a wider group given a belongs relation can be established) <span style="background-color: yellow; color: #d3163b;">and cognitive economy (less information needs to be stored about each specific entity, only its particularities)</span><span style="background-color: yellow; color: #d3163b;">. inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. for instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. one can consider fruit to be an abstraction of apple, orange, etc. conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant. an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code. inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor. complex inheritance, or inheritance used within a design that is not sufficiently mature, may lead to the yo-yo problem.</span></pre>
                </div>
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">in object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. the inheritance concept was invented in 1967 for simula</span><span style="background-color: yellow; color: #d3163b;"> inheritance provides the support for representation by categorization in computer languages. categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization </span><span style="background-color: yellow; color: #d3163b;">and cognitive economy (less information needs to be stored about each specific entity, only its particularities)</span><span style="background-color: yellow; color: #d3163b;">. the new classes, known as derived classes, take over (or inherit) attributes and behavior of the pre-existing classes, which are referred to as base classes (or ancestor classes). it is intended to help reuse existing code with little or no modification</span><span style="background-color: yellow; color: #d3163b;">. inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. for instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. one can consider fruit to be an abstraction of apple, orange, etc. conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant. an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code. inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor. complex inheritance, or inheritance used within a design that is not sufficiently mature, may lead to the yo-yo problem.</span></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>5. g0pE_taska.txt vs orig_taska.txt</description>
            <h2>Similarity: 93.84%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">in object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. the inheritance concept was invented in 1967 for simula. the new classes, known as derived classes, take over (or inherit) attribute</span><span style="background-color: yellow; color: #d3163b;"> and behavio</span>u<span style="background-color: yellow; color: #d3163b;">r of the pre-existing classes, which are referred to as base classes (or ancestor classes). it is intended to help reuse existing code with little or no modification. inheritance provides the support for representation by categorization in computer languages. categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization (what is known about specific entities is applied to a wider group given a belongs relation can be established) and cognitive economy (less information needs to be stored about each specific entity, only its particularities). inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. for instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. one can consider fruit to be an abstraction of apple, orange, etc. conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant. an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code. inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.</span></pre>
                </div>
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">in object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. the inheritance concept was invented in 1967 for simula. the new classes, known as derived classes, take over (or inherit) attribute</span>s<span style="background-color: yellow; color: #d3163b;"> and behavio</span><span style="background-color: yellow; color: #d3163b;">r of the pre-existing classes, which are referred to as base classes (or ancestor classes). it is intended to help reuse existing code with little or no modification. inheritance provides the support for representation by categorization in computer languages. categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization (what is known about specific entities is applied to a wider group given a belongs relation can be established) and cognitive economy (less information needs to be stored about each specific entity, only its particularities). inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. for instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. one can consider fruit to be an abstraction of apple, orange, etc. conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant. an advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code. inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.</span> complex inheritance, or inheritance used within a design that is not sufficiently mature, may lead to the yo-yo problem.</pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>6. orig_taskd.txt vs g4pC_taskd.txt</description>
            <h2>Similarity: 93.22%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">in probability theory, bayes' theorem</span> (often called bayes' law after rev thomas bayes)<span style="background-color: yellow; color: #d3163b;"> relates the conditional and marginal probabilities of two random events. it is </span>often<span style="background-color: yellow; color: #d3163b;"> used to compute posterior probabilities given observations. for </span>exampl<span style="background-color: yellow; color: #d3163b;">e, a patient may be observed to have certain symptoms. bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation.</span> (see example 2)<span style="background-color: yellow; color: #d3163b;"> as a formal theorem, bayes' theorem is valid in all common interpretations of probability. however, it plays a central role in the debate around the foundations of statistics: frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. </span><span style="background-color: yellow; color: #d3163b;">frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole</span>, while<span style="background-color: yellow; color: #d3163b;"> bayesians describe probabilities in terms of beliefs and degrees of uncertainty</span>. <span style="background-color: yellow; color: #d3163b;">the articles on bayesian probability and frequentist probability discuss these debates in greater detail</span><span style="background-color: yellow; color: #d3163b;">. bayes' theorem relates the conditional and marginal probabilities of events a and b, where b has a non-vanishing probability: </span>p(a|b) = \frac{p(b | a)\, p(a)}{p(b)}. <span style="background-color: yellow; color: #d3163b;">each term in bayes' theorem has a conventional name: </span>*<span style="background-color: yellow; color: #d3163b;"> p(a) is the prior probability or marginal probability of a. it is "prior" in the sense that it does not take into account any information about b. </span>*<span style="background-color: yellow; color: #d3163b;"> p(a|b) is the conditional probability of a, given b. it is also called the posterior probability because it is derived from or depends upon the specified value of b. </span>*<span style="background-color: yellow; color: #d3163b;"> p(b|a) is the conditional probability of b given a. </span>*<span style="background-color: yellow; color: #d3163b;"> p(b) is the prior or marginal probability of b, and acts as a normalizing constant. intuitively, bayes' theorem in this form describes the way in which one's beliefs about observing 'a' are updated by having observed 'b'.</span></pre>
                </div>
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">in probability theory, bayes' theorem</span><span style="background-color: yellow; color: #d3163b;"> relates the conditional and marginal probabilities of two random events. it is </span>usually be<span style="background-color: yellow; color: #d3163b;"> used to compute posterior probabilities given observations. for </span>instanc<span style="background-color: yellow; color: #d3163b;">e, a patient may be observed to have certain symptoms. bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation.</span><span style="background-color: yellow; color: #d3163b;"> as a formal theorem, bayes' theorem is valid in all common interpretations of probability. however, it plays a central role in the debate around the foundations of statistics: frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. </span><span style="background-color: yellow; color: #d3163b;">the articles on bayesian probability and frequentist probability discuss these debates in greater detail</span>. <span style="background-color: yellow; color: #d3163b;">frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole</span>. at the same time,<span style="background-color: yellow; color: #d3163b;"> bayesians describe probabilities in terms of beliefs and degrees of uncertainty</span><span style="background-color: yellow; color: #d3163b;">. bayes' theorem relates the conditional and marginal probabilities of events a and b, where b has a non-vanishing probability: </span><span style="background-color: yellow; color: #d3163b;">each term in bayes' theorem has a conventional name: </span>•<span style="background-color: yellow; color: #d3163b;"> p(a) is the prior probability or marginal probability of a. it is "prior" in the sense that it does not take into account any information about b. </span>•<span style="background-color: yellow; color: #d3163b;"> p(a|b) is the conditional probability of a, given b. it is also called the posterior probability because it is derived from or depends upon the specified value of b. </span>•<span style="background-color: yellow; color: #d3163b;"> p(b|a) is the conditional probability of b given a. </span>•<span style="background-color: yellow; color: #d3163b;"> p(b) is the prior or marginal probability of b, and acts as a normalizing constant. intuitively, bayes' theorem in this form describes the way in which one's beliefs about observing 'a' are updated by having observed 'b'.</span></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>7. orig_taskc.txt vs g2pA_taskc.txt</description>
            <h2>Similarity: 89.93%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">vector space model (or<span style="background-color: yellow; color: #d3163b;"> term</span> vector model) is an algebraic </span>model for<span style="background-color: yellow; color: #d3163b;"> representing text documents (and any objects, in general) as vectors of identifiers, such as</span>, for example,<span style="background-color: yellow; color: #d3163b;"> index<span style="background-color: yellow; color: #d3163b;"> term</span>s. it is used in information filtering, information retrieval, indexing and relevancy rankings. its first </span>use<span style="background-color: yellow; color: #d3163b;"> was in the smart information retrieval system. a document </span>is<span style="background-color: yellow; color: #d3163b;"> <span style="background-color: yellow; color: #d3163b;">represented</span> as a vector. e</span>ach<span style="background-color: yellow; color: #d3163b;"> dimension </span>correspond<span style="background-color: yellow; color: #d3163b;">s to a </span>separate<span style="background-color: yellow; color: #d3163b;"><span style="background-color: yellow; color: #d3163b;"> term</span>. if a<span style="background-color: yellow; color: #d3163b;"> term</span> </span>occu<span style="background-color: yellow; color: #d3163b;">rs in the document, </span>it<span style="background-color: yellow; color: #d3163b;">s value in the vector is non-zero. </span>s<span style="background-color: yellow; color: #d3163b;">ever</span>al<span style="background-color: yellow; color: #d3163b;"> different </span>way<span style="background-color: yellow; color: #d3163b;">s of c</span>ompu<span style="background-color: yellow; color: #d3163b;">ting these values, </span>also<span style="background-color: yellow; color: #d3163b;"> known as (term) weights, have been developed. </span><span style="background-color: yellow; color: #d3163b;">one of the </span>best<span style="background-color: yellow; color: #d3163b;"> known schemes</span> is <span style="background-color: yellow; color: #d3163b;">tf-idf weighting </span>(see the<span style="background-color: yellow; color: #d3163b;"> example </span><span style="background-color: yellow; color: #d3163b;">below</span><span style="background-color: yellow; color: #d3163b;">). the definition of</span><span style="background-color: yellow; color: #d3163b;"><span style="background-color: yellow; color: #d3163b;"> term</span> depends on the application. </span>typic<span style="background-color: yellow; color: #d3163b;">ally</span><span style="background-color: yellow; color: #d3163b;"> term</span>s are<span style="background-color: yellow; color: #d3163b;"> single word</span>s<span style="background-color: yellow; color: #d3163b;">, keyword</span>s<span style="background-color: yellow; color: #d3163b;">, or</span><span style="background-color: yellow; color: #d3163b;"> longer phrases</span><span style="background-color: yellow; color: #d3163b;">. if the words are chosen to be the<span style="background-color: yellow; color: #d3163b;"> term</span>s, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the corpus). the vector space model has </span>the following<span style="background-color: yellow; color: #d3163b;"> limitations: 1. long</span><span style="background-color: yellow; color: #d3163b;"> documents are </span><span style="background-color: yellow; color: #d3163b;">poorly </span><span style="background-color: yellow; color: #d3163b;">represented</span><span style="background-color: yellow; color: #d3163b;"> because the</span>y<span style="background-color: yellow; color: #d3163b;"> have poor similarity values (</span><span style="background-color: yellow; color: #d3163b;">a small scalar product and a large dimensionality) 2. search keywords </span>must<span style="background-color: yellow; color: #d3163b;"> precisely match document<span style="background-color: yellow; color: #d3163b;"> term</span>s; word substrings </span>might<span style="background-color: yellow; color: #d3163b;"> result in a "false positive match" 3. semantic sensitivity; documents with </span><span style="background-color: yellow; color: #d3163b;">similar context</span><span style="background-color: yellow; color: #d3163b;"> but<span style="background-color: yellow; color: #d3163b;"> different </span>term vocabulary won't be associated, resulting in a "false negative match". 4. the order in which t</span>he t<span style="background-color: yellow; color: #d3163b;">erms appear in the document is lost in </span>the<span style="background-color: yellow; color: #d3163b;"> vector space representation.</span></pre>
                </div>
                <div class="text-content">
                    <pre>a <span style="background-color: yellow; color: #d3163b;">vector space model (or<span style="background-color: yellow; color: #d3163b;"> term</span> vector model) is an algebraic </span>way of<span style="background-color: yellow; color: #d3163b;"> representing text documents (and any objects, in general) as vectors of identifiers, such as</span><span style="background-color: yellow; color: #d3163b;"> index<span style="background-color: yellow; color: #d3163b;"> term</span>s. it is used in information filtering, information retrieval, indexing and relevancy rankings. its first </span>application<span style="background-color: yellow; color: #d3163b;"> was in the smart information retrieval system. a document </span>can be<span style="background-color: yellow; color: #d3163b;"> <span style="background-color: yellow; color: #d3163b;">represented</span> as a vector. e</span>very<span style="background-color: yellow; color: #d3163b;"> dimension </span>relate<span style="background-color: yellow; color: #d3163b;">s to a </span>different<span style="background-color: yellow; color: #d3163b;"><span style="background-color: yellow; color: #d3163b;"> term</span>. if a<span style="background-color: yellow; color: #d3163b;"> term</span> </span>appea<span style="background-color: yellow; color: #d3163b;">rs in the document, </span>the<span style="background-color: yellow; color: #d3163b;"> term</span><span style="background-color: yellow; color: #d3163b;">s value in the vector is non-zero. </span>many<span style="background-color: yellow; color: #d3163b;"> different </span>method<span style="background-color: yellow; color: #d3163b;">s of c</span>alcula<span style="background-color: yellow; color: #d3163b;">ting these values, </span>sometimes<span style="background-color: yellow; color: #d3163b;"> known as (term) weights, have been developed. </span><span style="background-color: yellow; color: #d3163b;">tf-idf weighting </span>is <span style="background-color: yellow; color: #d3163b;">one of the </span>most well<span style="background-color: yellow; color: #d3163b;"> known schemes</span>.<span style="background-color: yellow; color: #d3163b;"> (see </span><span style="background-color: yellow; color: #d3163b;">below</span> example<span style="background-color: yellow; color: #d3163b;">). the definition of</span> a<span style="background-color: yellow; color: #d3163b;"><span style="background-color: yellow; color: #d3163b;"> term</span> depends on the application. </span>norm<span style="background-color: yellow; color: #d3163b;">ally</span> a<span style="background-color: yellow; color: #d3163b;"> term</span> is a<span style="background-color: yellow; color: #d3163b;"> single word</span><span style="background-color: yellow; color: #d3163b;">, keyword</span><span style="background-color: yellow; color: #d3163b;">, or</span> a longer phrase<span style="background-color: yellow; color: #d3163b;">. if the words are chosen to be the<span style="background-color: yellow; color: #d3163b;"> term</span>s, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the corpus). the vector space model has </span>some<span style="background-color: yellow; color: #d3163b;"> limitations: 1. long</span>er<span style="background-color: yellow; color: #d3163b;"> documents are </span><span style="background-color: yellow; color: #d3163b;">represented</span> poorly<span style="background-color: yellow; color: #d3163b;"> because the</span> documents<span style="background-color: yellow; color: #d3163b;"> have poor similarity values (</span>namely <span style="background-color: yellow; color: #d3163b;">a small scalar product and a large dimensionality) 2. search keywords </span>have to<span style="background-color: yellow; color: #d3163b;"> precisely match document<span style="background-color: yellow; color: #d3163b;"> term</span>s; word substrings </span>could potenti<span style="background-color: yellow; color: #d3163b;">ally</span><span style="background-color: yellow; color: #d3163b;"> result in a "false positive match" 3. semantic sensitivity; documents with </span>a <span style="background-color: yellow; color: #d3163b;">similar context</span>,<span style="background-color: yellow; color: #d3163b;"> but<span style="background-color: yellow; color: #d3163b;"> different </span>term vocabulary won't be associated, resulting in a "false negative match". 4. the order in which t</span><span style="background-color: yellow; color: #d3163b;">erms appear in the document is lost in </span>a<span style="background-color: yellow; color: #d3163b;"> vector space representation.</span></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>8. g3pC_taske.txt vs g0pE_taske.txt</description>
            <h2>Similarity: 87.14%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre>in computer science <span style="background-color: yellow; color: #d3163b;">and </span>mathematics, <span style="background-color: yellow; color: #d3163b;">dynamic programming is a method of </span>problem <span style="background-color: yellow; color: #d3163b;">solving</span><span style="background-color: yellow; color: #d3163b;"> that </span>utilises<span style="background-color: yellow; color: #d3163b;"> the properties of overlapping subproblems <span style="background-color: yellow; color: #d3163b;">and </span>optimal substructure</span>. <span style="background-color: yellow; color: #d3163b;">and </span>thus<span style="background-color: yellow; color: #d3163b;"> the method takes much less time than </span>more <span style="background-color: yellow; color: #d3163b;">naive methods.</span><span style="background-color: yellow; color: #d3163b;"> in "dynamic programming"</span>,<span style="background-color: yellow; color: #d3163b;"> the word "programming" has no </span>real<span style="background-color: yellow; color: #d3163b;"> connection to computer programming at all, </span>it<span style="background-color: yellow; color: #d3163b;"> act</span>ually<span style="background-color: yellow; color: #d3163b;"> comes from the term "mathematical programming", a synonym for optimi</span>s<span style="background-color: yellow; color: #d3163b;">ation. thus, the "program" is the optimal plan </span>of<span style="background-color: yellow; color: #d3163b;"><span style="background-color: yellow; color: #d3163b;"> act</span>ion<span style="background-color: yellow; color: #d3163b;"> that </span>is </span>being <span style="background-color: yellow; color: #d3163b;">produced. for </span>exampl<span style="background-color: yellow; color: #d3163b;">e, a</span><span style="background-color: yellow; color: #d3163b;"> schedule of events at an exhibition is sometimes called a program</span>me<span style="background-color: yellow; color: #d3163b;">. programming, in this sense, means finding an acceptable plan</span><span style="background-color: yellow; color: #d3163b;">, an algorithm.</span></pre>
                </div>
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">dynamic programming is a method of </span><span style="background-color: yellow; color: #d3163b;">solving</span><span style="background-color: yellow; color: #d3163b;"> problem</span>s<span style="background-color: yellow; color: #d3163b;"> that </span>exhibit<span style="background-color: yellow; color: #d3163b;"> the properties of overlapping subproblems <span style="background-color: yellow; color: #d3163b;">and </span>optimal substructure</span> (described below).<span style="background-color: yellow; color: #d3163b;"> the method takes much less time than </span><span style="background-color: yellow; color: #d3163b;">naive methods.</span> the word "programming"<span style="background-color: yellow; color: #d3163b;"> in "dynamic programming"</span> has no particular<span style="background-color: yellow; color: #d3163b;"> connection to computer programming at all, </span><span style="background-color: yellow; color: #d3163b;">and </span>instead<span style="background-color: yellow; color: #d3163b;"> comes from the term "mathematical programming", a synonym for optimi</span>z<span style="background-color: yellow; color: #d3163b;">ation. thus, the "program" is the optimal plan </span>for<span style="background-color: yellow; color: #d3163b;"><span style="background-color: yellow; color: #d3163b;"> act</span>ion<span style="background-color: yellow; color: #d3163b;"> that </span>is </span><span style="background-color: yellow; color: #d3163b;">produced. for </span>instanc<span style="background-color: yellow; color: #d3163b;">e, a</span> finalized<span style="background-color: yellow; color: #d3163b;"> schedule of events at an exhibition is sometimes called a program</span><span style="background-color: yellow; color: #d3163b;">. programming, in this sense, means finding an acceptable plan</span> of<span style="background-color: yellow; color: #d3163b;"> act</span>ion<span style="background-color: yellow; color: #d3163b;">, an algorithm.</span></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>9. orig_taskc.txt vs g0pA_taskc.txt</description>
            <h2>Similarity: 81.99%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">vector space model (</span>or<span style="background-color: yellow; color: #d3163b;"> term vector model) is an algebraic model </span>for<span style="background-color: yellow; color: #d3163b;"> represent</span>ing<span style="background-color: yellow; color: #d3163b;"> text documents</span> (and<span style="background-color: yellow; color: #d3163b;"> any objects</span>,<span style="background-color: yellow; color: #d3163b;"> in general</span>)<span style="background-color: yellow; color: #d3163b;"> as vectors of identifier</span>s, such<span style="background-color: yellow; color: #d3163b;"> as,</span> for example, index term<span style="background-color: yellow; color: #d3163b;">s. it is used in information </span>filtering, information <span style="background-color: yellow; color: #d3163b;">retrieval</span>, indexing<span style="background-color: yellow; color: #d3163b;"> and </span>relevancy rankings. it<span style="background-color: yellow; color: #d3163b;">s first use</span> was<span style="background-color: yellow; color: #d3163b;"> in<span style="background-color: yellow; color: #d3163b;"> the</span> smart information <span style="background-color: yellow; color: #d3163b;">retrieval</span> system. a document is <span style="background-color: yellow; color: #d3163b;">represented </span>as a vector</span>.<span style="background-color: yellow; color: #d3163b;"> each dimension corresponds to a separate term. if a term </span>occu<span style="background-color: yellow; color: #d3163b;">rs in<span style="background-color: yellow; color: #d3163b;"> the</span> document</span>,<span style="background-color: yellow; color: #d3163b;"> its value in<span style="background-color: yellow; color: #d3163b;"> the</span> vector is non-zero. </span><span style="background-color: yellow; color: #d3163b;">several</span><span style="background-color: yellow; color: #d3163b;"> different ways of c</span>ompu<span style="background-color: yellow; color: #d3163b;">ting<span style="background-color: yellow; color: #d3163b;"> the</span>se values, also known as (term) weights, have been developed. one of<span style="background-color: yellow; color: #d3163b;"> the</span> best known </span>scheme<span style="background-color: yellow; color: #d3163b;">s is</span><span style="background-color: yellow; color: #d3163b;"> tf-idf weighting </span>(see<span style="background-color: yellow; color: #d3163b;"> the</span> example below)<span style="background-color: yellow; color: #d3163b;">.<span style="background-color: yellow; color: #d3163b;"> the</span> definition of term depends on<span style="background-color: yellow; color: #d3163b;"> the</span> application</span>. typic<span style="background-color: yellow; color: #d3163b;">ally terms are single<span style="background-color: yellow; color: #d3163b;"> wor</span>ds, keywords, or longer phrases. if<span style="background-color: yellow; color: #d3163b;"> the</span><span style="background-color: yellow; color: #d3163b;"> wor</span>ds are chosen to be<span style="background-color: yellow; color: #d3163b;"> the</span> terms,<span style="background-color: yellow; color: #d3163b;"> the</span> dimensionality of<span style="background-color: yellow; color: #d3163b;"> the</span> vector is<span style="background-color: yellow; color: #d3163b;"> the</span> number of<span style="background-color: yellow; color: #d3163b;"> wor</span>ds in<span style="background-color: yellow; color: #d3163b;"> the</span> vocabulary</span> (<span style="background-color: yellow; color: #d3163b;"><span style="background-color: yellow; color: #d3163b;">the </span>number of distinct<span style="background-color: yellow; color: #d3163b;"> wor</span>ds occurring in<span style="background-color: yellow; color: #d3163b;"> the</span> corpus</span>)<span style="background-color: yellow; color: #d3163b;">.<span style="background-color: yellow; color: #d3163b;"> the</span> vector space model has </span><span style="background-color: yellow; color: #d3163b;">the </span>following limitations: 1.<span style="background-color: yellow; color: #d3163b;"> long documents are </span>poorly<span style="background-color: yellow; color: #d3163b;"> represent</span>ed<span style="background-color: yellow; color: #d3163b;"> because<span style="background-color: yellow; color: #d3163b;"> the</span>y have poor similarity values</span> (a small scalar product<span style="background-color: yellow; color: #d3163b;"> and </span>a large dimensionality) 2.<span style="background-color: yellow; color: #d3163b;"> search keywords must </span>precis<span style="background-color: yellow; color: #d3163b;">ely match document terms</span>;<span style="background-color: yellow; color: #d3163b;"> wor</span><span style="background-color: yellow; color: #d3163b;">d substring</span><span style="background-color: yellow; color: #d3163b;">s might result in a "false</span> <span style="background-color: yellow; color: #d3163b;">positive match"</span> 3<span style="background-color: yellow; color: #d3163b;">. se</span>mantic sensitivity;<span style="background-color: yellow; color: #d3163b;"> documents with similar context but different term vocabulary w</span>on'<span style="background-color: yellow; color: #d3163b;">t be associated, resulting in a "false</span> <span style="background-color: yellow; color: #d3163b;">negative match". </span>4.<span style="background-color: yellow; color: #d3163b;"><span style="background-color: yellow; color: #d3163b;"> the</span> order in which<span style="background-color: yellow; color: #d3163b;"> the</span> terms appear in<span style="background-color: yellow; color: #d3163b;"> the</span> document is lost in<span style="background-color: yellow; color: #d3163b;"> the</span> vector space<span style="background-color: yellow; color: #d3163b;"> represent</span>ation.</span></pre>
                </div>
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">the </span><span style="background-color: yellow; color: #d3163b;">vector space model (</span>also called,<span style="background-color: yellow; color: #d3163b;"> term vector model) is an algebraic model </span>used to<span style="background-color: yellow; color: #d3163b;"> represent</span><span style="background-color: yellow; color: #d3163b;"> text documents</span>, as well as<span style="background-color: yellow; color: #d3163b;"> any objects</span><span style="background-color: yellow; color: #d3163b;"> in general</span>,<span style="background-color: yellow; color: #d3163b;"> as vectors of identifier</span><span style="background-color: yellow; color: #d3163b;">s. it is used in information </span><span style="background-color: yellow; color: #d3163b;">retrieval</span><span style="background-color: yellow; color: #d3163b;"> and </span>wa<span style="background-color: yellow; color: #d3163b;">s first use</span>d<span style="background-color: yellow; color: #d3163b;"> in<span style="background-color: yellow; color: #d3163b;"> the</span> smart information <span style="background-color: yellow; color: #d3163b;">retrieval</span> system. a document is <span style="background-color: yellow; color: #d3163b;">represented </span>as a vector</span> and<span style="background-color: yellow; color: #d3163b;"> each dimension corresponds to a separate term. if a term </span>appea<span style="background-color: yellow; color: #d3163b;">rs in<span style="background-color: yellow; color: #d3163b;"> the</span> document</span><span style="background-color: yellow; color: #d3163b;"> the</span>n<span style="background-color: yellow; color: #d3163b;"> its value in<span style="background-color: yellow; color: #d3163b;"> the</span> vector is non-zero. </span>many<span style="background-color: yellow; color: #d3163b;"> different ways of c</span>alcula<span style="background-color: yellow; color: #d3163b;">ting<span style="background-color: yellow; color: #d3163b;"> the</span>se values, also known as (term) weights, have been developed. one of<span style="background-color: yellow; color: #d3163b;"> the</span> best known </span>method<span style="background-color: yellow; color: #d3163b;">s is</span> called tf-idf weighting<span style="background-color: yellow; color: #d3163b;">.<span style="background-color: yellow; color: #d3163b;"> the</span> definition of term depends on<span style="background-color: yellow; color: #d3163b;"> the</span> application</span> but gener<span style="background-color: yellow; color: #d3163b;">ally terms are single<span style="background-color: yellow; color: #d3163b;"> wor</span>ds, keywords, or longer phrases. if<span style="background-color: yellow; color: #d3163b;"> the</span><span style="background-color: yellow; color: #d3163b;"> wor</span>ds are chosen to be<span style="background-color: yellow; color: #d3163b;"> the</span> terms,<span style="background-color: yellow; color: #d3163b;"> the</span> dimensionality of<span style="background-color: yellow; color: #d3163b;"> the</span> vector is<span style="background-color: yellow; color: #d3163b;"> the</span> number of<span style="background-color: yellow; color: #d3163b;"> wor</span>ds in<span style="background-color: yellow; color: #d3163b;"> the</span> vocabulary</span>, which is <span style="background-color: yellow; color: #d3163b;"><span style="background-color: yellow; color: #d3163b;">the </span>number of distinct<span style="background-color: yellow; color: #d3163b;"> wor</span>ds occurring in<span style="background-color: yellow; color: #d3163b;"> the</span> corpus</span><span style="background-color: yellow; color: #d3163b;">.<span style="background-color: yellow; color: #d3163b;"> the</span> vector space model has </span><span style="background-color: yellow; color: #d3163b;">several</span> disadvantages. firstly,<span style="background-color: yellow; color: #d3163b;"> long documents are </span><span style="background-color: yellow; color: #d3163b;">represented </span>badly<span style="background-color: yellow; color: #d3163b;"> because<span style="background-color: yellow; color: #d3163b;"> the</span>y have poor similarity values</span><span style="background-color: yellow; color: #d3163b;">. se</span>condly,<span style="background-color: yellow; color: #d3163b;"> search keywords must </span>accurat<span style="background-color: yellow; color: #d3163b;">ely match document terms</span> an<span style="background-color: yellow; color: #d3163b;">d substring</span>s of<span style="background-color: yellow; color: #d3163b;"> wor</span>d<span style="background-color: yellow; color: #d3163b;">s might result in a "false</span>-<span style="background-color: yellow; color: #d3163b;">positive match"</span>. thirdly,<span style="background-color: yellow; color: #d3163b;"> documents with similar context but different term vocabulary w</span>ill no<span style="background-color: yellow; color: #d3163b;">t be associated, resulting in a "false</span>-<span style="background-color: yellow; color: #d3163b;">negative match". </span>finally,<span style="background-color: yellow; color: #d3163b;"><span style="background-color: yellow; color: #d3163b;"> the</span> order in which<span style="background-color: yellow; color: #d3163b;"> the</span> terms appear in<span style="background-color: yellow; color: #d3163b;"> the</span> document is lost in<span style="background-color: yellow; color: #d3163b;"> the</span> vector space<span style="background-color: yellow; color: #d3163b;"> represent</span>ation.</span></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>10. g4pC_taskd.txt vs g2pB_taskd.txt</description>
            <h2>Similarity: 80.72%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">in probability theory,<span style="background-color: yellow; color: #d3163b;"> bayes' </span>theorem </span>relat<span style="background-color: yellow; color: #d3163b;">es the conditional and marginal probabilities of two random events. it is </span>usually be<span style="background-color: yellow; color: #d3163b;"> used to c</span>ompu<span style="background-color: yellow; color: #d3163b;">te posterior probabilities given observations. for </span>instanc<span style="background-color: yellow; color: #d3163b;">e, a patient may be observed to have certain symptoms.<span style="background-color: yellow; color: #d3163b;"> bayes' </span>theorem can be<span style="background-color: yellow; color: #d3163b;"> used to c</span></span>ompu<span style="background-color: yellow; color: #d3163b;">te the </span>probability<span style="background-color: yellow; color: #d3163b;"> that a proposed </span>diagno<span style="background-color: yellow; color: #d3163b;">sis is </span>correct<span style="background-color: yellow; color: #d3163b;">, given that observation. as a</span> form<span style="background-color: yellow; color: #d3163b;">al theorem,<span style="background-color: yellow; color: #d3163b;"> bayes' </span>theorem is valid in all </span>common<span style="background-color: yellow; color: #d3163b;"> interpretations of probability. however, it plays a </span>centr<span style="background-color: yellow; color: #d3163b;">al role in the debate around the foundations of statistics: frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. </span><span style="background-color: yellow; color: #d3163b;">the articles on bayesian probability and frequentist probability discuss these debates in greater detail. </span><span style="background-color: yellow; color: #d3163b;">frequentists assign probabilities to random events according to their frequencies of </span>o<span style="background-color: yellow; color: #d3163b;">ccur</span>rence<span style="background-color: yellow; color: #d3163b;"> or to subsets of populations as proportions of the whole. </span>at the same time,<span style="background-color: yellow; color: #d3163b;"> bayesians describe probabilities in terms of beliefs and degrees of uncertainty.<span style="background-color: yellow; color: #d3163b;"> bayes' </span>theorem </span>relat<span style="background-color: yellow; color: #d3163b;">es the conditional and marginal probabilities of events a and b, where b has a non-vanishing probability</span>:<span style="background-color: yellow; color: #d3163b;"> each term in<span style="background-color: yellow; color: #d3163b;"> bayes' </span>theorem has a conventional name: </span>• <span style="background-color: yellow; color: #d3163b;">p(a) is the pr</span>ior probability or marginal<span style="background-color: yellow; color: #d3163b;"> probability of a. it is "pr</span>ior<span style="background-color: yellow; color: #d3163b;">" in the sense that it does not take into account any information about b.</span> •<span style="background-color: yellow; color: #d3163b;"> p(a|b) is the conditional probability of a, given b. it is also called the </span>posterior<span style="background-color: yellow; color: #d3163b;"> probability because it is derived from or depends upon the specified value of b. </span>• <span style="background-color: yellow; color: #d3163b;">p(b|a) is the conditional probability of b given a. </span>• <span style="background-color: yellow; color: #d3163b;">p(b) is the pr</span>ior or marginal probability of b, and acts as a normalizing constant. intuitively,<span style="background-color: yellow; color: #d3163b;"> bayes' </span>theorem in this form describes the way in which one's beliefs about observing 'a' are updated by having observed 'b'.</pre>
                </div>
                <div class="text-content">
                    <pre><span style="background-color: yellow; color: #d3163b;">in probability theory,<span style="background-color: yellow; color: #d3163b;"> bayes' </span>theorem </span>also called<span style="background-color: yellow; color: #d3163b;"> bayes' </span>law after rev thomas bayes <span style="background-color: yellow; color: #d3163b;">comp</span>ar<span style="background-color: yellow; color: #d3163b;">es the conditional and marginal probabilities of two random events. it is </span>often<span style="background-color: yellow; color: #d3163b;"> used to c</span>alcula<span style="background-color: yellow; color: #d3163b;">te posterior probabilities given observations. for </span>exampl<span style="background-color: yellow; color: #d3163b;">e, a patient may be observed to have certain symptoms.<span style="background-color: yellow; color: #d3163b;"> bayes' </span>theorem can be<span style="background-color: yellow; color: #d3163b;"> used to c</span></span>alcula<span style="background-color: yellow; color: #d3163b;">te the </span>likelihood<span style="background-color: yellow; color: #d3163b;"> that a proposed </span>analy<span style="background-color: yellow; color: #d3163b;">sis is </span>a<span style="background-color: yellow; color: #d3163b;">ccur</span>ate<span style="background-color: yellow; color: #d3163b;">, given that observation. as a</span>n offici<span style="background-color: yellow; color: #d3163b;">al theorem,<span style="background-color: yellow; color: #d3163b;"> bayes' </span>theorem is valid in all </span>universal<span style="background-color: yellow; color: #d3163b;"> interpretations of probability. however, it plays a </span>fundament<span style="background-color: yellow; color: #d3163b;">al role in the debate around the foundations of statistics: frequentist and bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. </span><span style="background-color: yellow; color: #d3163b;">frequentists assign probabilities to random events according to their frequencies of </span>happening<span style="background-color: yellow; color: #d3163b;"> or to subsets of populations as proportions of the whole. </span>whilst bayesians describe probabilities in terms of beliefs and degrees of uncertainty. <span style="background-color: yellow; color: #d3163b;">the articles on bayesian probability and frequentist probability discuss these debates in greater detail. </span>bayes' theorem <span style="background-color: yellow; color: #d3163b;">comp</span>ar<span style="background-color: yellow; color: #d3163b;">es the conditional and marginal probabilities of events a and b, where b has a non-vanishing probability</span>.<span style="background-color: yellow; color: #d3163b;"> each term in<span style="background-color: yellow; color: #d3163b;"> bayes' </span>theorem has a conventional name: </span><span style="background-color: yellow; color: #d3163b;">p(a) is the pr</span>evious<span style="background-color: yellow; color: #d3163b;"> probability of a. it is "pr</span>evious<span style="background-color: yellow; color: #d3163b;">" in the sense that it does not take into account any information about b.</span><span style="background-color: yellow; color: #d3163b;"> p(a|b) is the conditional probability of a, given b. it is also called the </span>subsequent<span style="background-color: yellow; color: #d3163b;"> probability because it is derived from or depends upon the specified value of b. </span><span style="background-color: yellow; color: #d3163b;">p(b|a) is the conditional probability of b given a. </span><span style="background-color: yellow; color: #d3163b;">p(b) is the pr</span>evious.</pre>
                </div>
            </div>
        </div></body></html>