<html>
<head>
<title>Plagiarism Detector</title>
<style>

@font-face {
   font-family: Nohemi;
   src: url("Nohemi-Medium.otf")
}

@font-face {
   font-family: OffBit;
   src: url("OffBit.ttf")
}

body {
   background-color: #ffa1d7;
   font-family: Nohemi;
   color: #d3163b;
   margin: 0;
   padding: 0;
   box-sizing: border-box;
}

.container {
   max-width: 1370px;
   margin: 20px auto;
   padding: 20px;
   border: 1px solid #d3163b;
   background-color: #fdf2f5;
   box-sizing: border-box;
}

pre {
   border: 1px solid #d3163b;
   padding: 10px;
   background-color: #f8f7f8; 
   width: 100%;
   white-space: pre-wrap;
   word-wrap: break-word;
   box-sizing: border-box;
}

h1 {
   font-size: 60px;
   text-align: center;
   margin-top: 50px;
   font-family: OffBit;
}

h2 {
   margin-top: 5px;
   margin-bottom: 5px;
   font-size: 20px;
}

h3 {
   margin-top: 0px;
   font-size: 22px;
   margin-bottom: 45px;
   margin-left: 60px;
   margin-right: 60px;
}

description {
   margin-top: 0px;
   font-size: 25px;
   margin-bottom: 45px;
   font-family: OffBit;
}

.similarity-box {
   margin-bottom: 20px;
}

.text-container {
   display: block; 
}

.highlight {
   background-color: yellow;
   color: #d3163b;
}
</style>
</head>
<body>
<h1>Plagiarism Detector •*.✸ </h1>
<h3 style="text-align: center;">Web App in Golang that compares the content of different texts about the same topic, calculating and highlighting their similarity using a Suffix Array that retrieves the Longest Common Substring (LCS).</h3>
        <div class="container">
            <description>1. orig_taskd.txt vs g3pA_taskd.txt</description>
            <h2>Similarity: 96.68%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">In probability theory, Bayes' theorem (often called Bayes' law after Rev Thomas Bayes) relates the conditional and marginal probabilities of two random events. It is often used to compute posterior probabilities given observations</span>. F<span style="background-color: yellow; color: #d3163b;">or example, a patient may be observed to have certain symptoms</span><span style="background-color: yellow; color: #d3163b;">. Bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation.</span> (See example 2)<span style="background-color: yellow; color: #d3163b;">As a formal theorem, Bayes' theorem is valid in all common interpretations of probability. However, it plays a central role in the debate around the foundations of statistics</span>:<span style="background-color: yellow; color: #d3163b;"> frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. Frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole, while Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. The articles on Bayesian probability and frequentist probability discuss these debates in greater detail.Bayes' theorem relates the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability:    P(A|B) = </span>\frac{<span style="background-color: yellow; color: #d3163b;">P(B | A)</span>\, P(A)}{P(B)}<span style="background-color: yellow; color: #d3163b;">.Each term in Bayes' theorem has a conventional name:</span>    * <span style="background-color: yellow; color: #d3163b;">P(A) is the prior probability or marginal probability of A. It is "prior" in the sense that it does not take into account any information about B.</span>    *<span style="background-color: yellow; color: #d3163b;"> P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from or depends upon the specified value of B.</span>    * <span style="background-color: yellow; color: #d3163b;">P(B|A) is the conditional probability of B given A.</span>    * <span style="background-color: yellow; color: #d3163b;">P(B) is the prior or marginal probability of B, and acts as a normalizing constant.Intuitively, Bayes' theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.</span></pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">In probability theory, Bayes' theorem (often called Bayes' law after Rev Thomas Bayes) relates the conditional and marginal probabilities of two random events. It is often used to compute posterior probabilities given observations</span> (f<span style="background-color: yellow; color: #d3163b;">or example, a patient may be observed to have certain symptoms</span>)<span style="background-color: yellow; color: #d3163b;">. Bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation.</span><span style="background-color: yellow; color: #d3163b;">As a formal theorem, Bayes' theorem is valid in all common interpretations of probability. However, it plays a central role in the debate around the foundations of statistics</span>;<span style="background-color: yellow; color: #d3163b;"> frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. Frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole, while Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. The articles on Bayesian probability and frequentist probability discuss these debates in greater detail.Bayes' theorem relates the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability:    P(A|B) = </span>(<span style="background-color: yellow; color: #d3163b;">P(B | A)</span> x P(A)) / P(B)<span style="background-color: yellow; color: #d3163b;">.Each term in Bayes' theorem has a conventional name:</span><span style="background-color: yellow; color: #d3163b;">P(A) is the prior probability or marginal probability of A. It is "prior" in the sense that it does not take into account any information about B.</span><span style="background-color: yellow; color: #d3163b;"> P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from or depends upon the specified value of B.</span><span style="background-color: yellow; color: #d3163b;">P(B|A) is the conditional probability of B given A.</span><span style="background-color: yellow; color: #d3163b;">P(B) is the prior or marginal probability of B, and acts as a normalizing constant.Intuitively, Bayes' theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.</span></pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>2. orig_taska.txt vs g4pC_taska.txt</description>
            <h2>Similarity: 94.28%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula</span><span style="background-color: yellow; color: #d3163b;">.The new classes, known as derived classes, take over (or inherit) attributes and behavior of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification</span>.<span style="background-color: yellow; color: #d3163b;">Inheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization </span>(what is known about specific entities is applied to a wider group given a belongs relation can be established) <span style="background-color: yellow; color: #d3163b;">and cognitive economy (less information needs to be stored about each specific entity, only its particularities)</span><span style="background-color: yellow; color: #d3163b;">.Inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant.An advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code.Inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.Complex inheritance, or inheritance used within a design that is not sufficiently mature, may lead to the Yo-yo problem.</span></pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula</span><span style="background-color: yellow; color: #d3163b;">Inheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization </span><span style="background-color: yellow; color: #d3163b;">and cognitive economy (less information needs to be stored about each specific entity, only its particularities)</span><span style="background-color: yellow; color: #d3163b;">.The new classes, known as derived classes, take over (or inherit) attributes and behavior of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification</span><span style="background-color: yellow; color: #d3163b;">.Inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant.An advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code.Inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.Complex inheritance, or inheritance used within a design that is not sufficiently mature, may lead to the Yo-yo problem.</span></pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>3. g3pA_taskd.txt vs g4pC_taskd.txt</description>
            <h2>Similarity: 93.97%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">In probability theory, Bayes' theorem</span> (often called Bayes' law after Rev Thomas Bayes)<span style="background-color: yellow; color: #d3163b;"> relates the conditional and marginal probabilities of two random events. It is </span>often<span style="background-color: yellow; color: #d3163b;"> used to compute posterior probabilities given observations</span> (for exampl<span style="background-color: yellow; color: #d3163b;">e, a patient may be observed to have certain symptoms</span>)<span style="background-color: yellow; color: #d3163b;">. Bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation.</span><span style="background-color: yellow; color: #d3163b;">As a formal theorem, Bayes' theorem is valid in all common interpretations of probability. However, it plays a central role in the debate around the foundations of statistics</span>;<span style="background-color: yellow; color: #d3163b;"> frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications</span><span style="background-color: yellow; color: #d3163b;">. Frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole</span>, while<span style="background-color: yellow; color: #d3163b;"> Bayesians describe probabilities in terms of beliefs and degrees of uncertainty</span>. The articles on Bayesian probability and frequentist probability discuss these debates in greater detail.<span style="background-color: yellow; color: #d3163b;">Bayes' theorem relates the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability:</span>    P(A|B) = (P(B | A) x P(A)) / P(B).<span style="background-color: yellow; color: #d3163b;">Each term in Bayes' theorem has a conventional name:</span><span style="background-color: yellow; color: #d3163b;">P(A) is the prior probability or marginal probability of A. It is "prior" in the sense that it does not take into account any information about B. </span><span style="background-color: yellow; color: #d3163b;">P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from or depends upon the specified value of B.</span><span style="background-color: yellow; color: #d3163b;">P(B|A) is the conditional probability of B given A.</span><span style="background-color: yellow; color: #d3163b;">P(B) is the prior or marginal probability of B, and acts as a normalizing constant.</span><span style="background-color: yellow; color: #d3163b;">Intuitively, Bayes' theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.</span></pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">In probability theory, Bayes' theorem</span><span style="background-color: yellow; color: #d3163b;"> relates the conditional and marginal probabilities of two random events. It is </span>usually be<span style="background-color: yellow; color: #d3163b;"> used to compute posterior probabilities given observations</span>. For instanc<span style="background-color: yellow; color: #d3163b;">e, a patient may be observed to have certain symptoms</span><span style="background-color: yellow; color: #d3163b;">. Bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation.</span> <span style="background-color: yellow; color: #d3163b;">As a formal theorem, Bayes' theorem is valid in all common interpretations of probability. However, it plays a central role in the debate around the foundations of statistics</span>:<span style="background-color: yellow; color: #d3163b;"> frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications</span>. The articles on Bayesian probability and frequentist probability discuss these debates in greater detail<span style="background-color: yellow; color: #d3163b;">. Frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole</span>. At the same time, <span style="background-color: yellow; color: #d3163b;"> Bayesians describe probabilities in terms of beliefs and degrees of uncertainty</span>. <span style="background-color: yellow; color: #d3163b;">Bayes' theorem relates the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability:</span><span style="background-color: yellow; color: #d3163b;">Each term in Bayes' theorem has a conventional name:</span>•<span style="background-color: yellow; color: #d3163b;">P(A) is the prior probability or marginal probability of A. It is "prior" in the sense that it does not take into account any information about B. </span>•<span style="background-color: yellow; color: #d3163b;">P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from or depends upon the specified value of B.</span> •<span style="background-color: yellow; color: #d3163b;">P(B|A) is the conditional probability of B given A.</span> •<span style="background-color: yellow; color: #d3163b;">P(B) is the prior or marginal probability of B, and acts as a normalizing constant.</span> <span style="background-color: yellow; color: #d3163b;">Intuitively, Bayes' theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.</span></pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>4. g0pE_taska.txt vs g4pC_taska.txt</description>
            <h2>Similarity: 93.88%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula</span>. <span style="background-color: yellow; color: #d3163b;">The new classes, known as derived classes, take over (or inherit) attribute</span><span style="background-color: yellow; color: #d3163b;"> and behavio</span>u<span style="background-color: yellow; color: #d3163b;">r of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification.</span> <span style="background-color: yellow; color: #d3163b;">Inheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization </span>(w<span style="background-color: yellow; color: #d3163b;">hat is </span>known about specific entities is applied to a wider group given a belongs relation can be established) <span style="background-color: yellow; color: #d3163b;">and cognitive economy (less information needs to be stored about each specific entity, only its particularities).</span> <span style="background-color: yellow; color: #d3163b;">Inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant.</span> <span style="background-color: yellow; color: #d3163b;">An advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code.</span> <span style="background-color: yellow; color: #d3163b;">Inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.</span></pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula</span><span style="background-color: yellow; color: #d3163b;">Inheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization </span><span style="background-color: yellow; color: #d3163b;">and cognitive economy (less information needs to be stored about each specific entity, only its particularities).</span><span style="background-color: yellow; color: #d3163b;">The new classes, known as derived classes, take over (or inherit) attribute</span>s<span style="background-color: yellow; color: #d3163b;"> and behavio</span><span style="background-color: yellow; color: #d3163b;">r of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification.</span><span style="background-color: yellow; color: #d3163b;">Inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant.</span><span style="background-color: yellow; color: #d3163b;">An advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code.</span><span style="background-color: yellow; color: #d3163b;">Inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.</span>Complex inheritance, or inheritance used within a design t<span style="background-color: yellow; color: #d3163b;">hat is </span>not sufficiently mature, may lead to the Yo-yo problem.</pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>5. g0pE_taska.txt vs orig_taska.txt</description>
            <h2>Similarity: 93.87%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula.</span> <span style="background-color: yellow; color: #d3163b;">The new classes, known as derived classes, take over (or inherit) attribute</span><span style="background-color: yellow; color: #d3163b;"> and behavio</span>u<span style="background-color: yellow; color: #d3163b;">r of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification.</span> <span style="background-color: yellow; color: #d3163b;">Inheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization (what is known about specific entities is applied to a wider group given a belongs relation can be established) and cognitive economy (less information needs to be stored about each specific entity, only its particularities).</span> <span style="background-color: yellow; color: #d3163b;">Inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant.</span> <span style="background-color: yellow; color: #d3163b;">An advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code.</span> <span style="background-color: yellow; color: #d3163b;">Inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.</span></pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula.</span><span style="background-color: yellow; color: #d3163b;">The new classes, known as derived classes, take over (or inherit) attribute</span>s<span style="background-color: yellow; color: #d3163b;"> and behavio</span><span style="background-color: yellow; color: #d3163b;">r of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification.</span><span style="background-color: yellow; color: #d3163b;">Inheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization (what is known about specific entities is applied to a wider group given a belongs relation can be established) and cognitive economy (less information needs to be stored about each specific entity, only its particularities).</span><span style="background-color: yellow; color: #d3163b;">Inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant.</span><span style="background-color: yellow; color: #d3163b;">An advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code.</span><span style="background-color: yellow; color: #d3163b;">Inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.</span>Complex inheritance, or inheritance used within a design that is not sufficiently mature, may lead to the Yo-yo problem.</pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>6. orig_taskd.txt vs g4pC_taskd.txt</description>
            <h2>Similarity: 92.42%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">In probability theory, Bayes' theorem</span> (often called Bayes' law after Rev Thomas Bayes)<span style="background-color: yellow; color: #d3163b;"> relates the conditional and marginal probabilities of two random events. It is </span>often<span style="background-color: yellow; color: #d3163b;"> used to compute posterior probabilities given observations. For </span>exampl<span style="background-color: yellow; color: #d3163b;">e, a patient may be observed to have certain symptoms. Bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation. </span>(See example 2)<span style="background-color: yellow; color: #d3163b;">As a formal theorem, Bayes' theorem is valid in all common interpretations of probability. However, it plays a central role in the debate around the foundations of statistics: frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. </span><span style="background-color: yellow; color: #d3163b;">Frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole</span>, while<span style="background-color: yellow; color: #d3163b;"> Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. </span>The articles on Bayesian probability and frequentist probability discuss these debates in greater detail.<span style="background-color: yellow; color: #d3163b;">Bayes' theorem relates the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability:</span>    P(A|B) = \frac{P(B | A)\, P(A)}{P(B)}.<span style="background-color: yellow; color: #d3163b;">Each term in Bayes' theorem has a conventional name:</span>    * <span style="background-color: yellow; color: #d3163b;">P(A) is the prior probability or marginal probability of A. It is "prior" in the sense that it does not take into account any information about B. </span>   * <span style="background-color: yellow; color: #d3163b;">P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from or depends upon the specified value of B. </span>   * <span style="background-color: yellow; color: #d3163b;">P(B|A) is the conditional probability of B given A. </span>   * <span style="background-color: yellow; color: #d3163b;">P(B) is the prior or marginal probability of B, and acts as a normalizing constant.</span><span style="background-color: yellow; color: #d3163b;">Intuitively, Bayes' theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.</span></pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">In probability theory, Bayes' theorem</span><span style="background-color: yellow; color: #d3163b;"> relates the conditional and marginal probabilities of two random events. It is </span>usually be<span style="background-color: yellow; color: #d3163b;"> used to compute posterior probabilities given observations. For </span>instanc<span style="background-color: yellow; color: #d3163b;">e, a patient may be observed to have certain symptoms. Bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation. </span><span style="background-color: yellow; color: #d3163b;">As a formal theorem, Bayes' theorem is valid in all common interpretations of probability. However, it plays a central role in the debate around the foundations of statistics: frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. </span><span style="background-color: yellow; color: #d3163b;">The articles on Bayesian probability and frequentist probability discuss these debates in greater detail. </span><span style="background-color: yellow; color: #d3163b;">Frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole</span>. At the same time, <span style="background-color: yellow; color: #d3163b;"> Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. </span><span style="background-color: yellow; color: #d3163b;">Bayes' theorem relates the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability:</span><span style="background-color: yellow; color: #d3163b;">Each term in Bayes' theorem has a conventional name:</span>•<span style="background-color: yellow; color: #d3163b;">P(A) is the prior probability or marginal probability of A. It is "prior" in the sense that it does not take into account any information about B. </span>•<span style="background-color: yellow; color: #d3163b;">P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from or depends upon the specified value of B. </span>•<span style="background-color: yellow; color: #d3163b;">P(B|A) is the conditional probability of B given A. </span>•<span style="background-color: yellow; color: #d3163b;">P(B) is the prior or marginal probability of B, and acts as a normalizing constant.</span> <span style="background-color: yellow; color: #d3163b;">Intuitively, Bayes' theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.</span></pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>7. orig_taskc.txt vs g2pA_taskc.txt</description>
            <h2>Similarity: 87.63%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">Vector space model (or term vector model) is an algebraic </span>model for<span style="background-color: yellow; color: #d3163b;"> representing text documents (and any objects, in general) as vectors of identifiers, such as</span>, for example,<span style="background-color: yellow; color: #d3163b;"> index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings. Its first </span>use<span style="background-color: yellow; color: #d3163b;"> was in the SMART Information Retrieval System.A document </span>is<span style="background-color: yellow; color: #d3163b;"> <span style="background-color: yellow; color: #d3163b;">represented</span> as a vector. E</span>ach<span style="background-color: yellow; color: #d3163b;"> dimension </span>correspond<span style="background-color: yellow; color: #d3163b;">s to a </span>separate<span style="background-color: yellow; color: #d3163b;"> term. If a term </span>occu<span style="background-color: yellow; color: #d3163b;">rs in the document, </span>it<span style="background-color: yellow; color: #d3163b;">s value in the vector is non-zero. </span>Several<span style="background-color: yellow; color: #d3163b;"> different </span>way<span style="background-color: yellow; color: #d3163b;">s of c</span>ompu<span style="background-color: yellow; color: #d3163b;">ting these values, </span>also<span style="background-color: yellow; color: #d3163b;"> known as (term) weights, have been developed. </span>O<span style="background-color: yellow; color: #d3163b;">ne of the </span>best<span style="background-color: yellow; color: #d3163b;"> known schemes</span> is <span style="background-color: yellow; color: #d3163b;">tf-idf weighting </span>(see the<span style="background-color: yellow; color: #d3163b;"> example </span>below<span style="background-color: yellow; color: #d3163b;">).The definition of</span><span style="background-color: yellow; color: #d3163b;"> term depends on the application. </span>Typically terms are<span style="background-color: yellow; color: #d3163b;"> single word</span>s<span style="background-color: yellow; color: #d3163b;">, keyword</span>s, or<span style="background-color: yellow; color: #d3163b;"> longer phrases</span><span style="background-color: yellow; color: #d3163b;">. If the words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the corpus).The vector space model has </span>the following<span style="background-color: yellow; color: #d3163b;"> limitations:</span>   1. Long<span style="background-color: yellow; color: #d3163b;"> documents are </span><span style="background-color: yellow; color: #d3163b;">poorly </span><span style="background-color: yellow; color: #d3163b;">represented</span><span style="background-color: yellow; color: #d3163b;"> because the</span>y<span style="background-color: yellow; color: #d3163b;"> have poor similarity values (</span><span style="background-color: yellow; color: #d3163b;">a small scalar product and a large dimensionality)</span>   2. <span style="background-color: yellow; color: #d3163b;">Search keywords </span>must<span style="background-color: yellow; color: #d3163b;"> precisely match document terms; word substrings </span>might<span style="background-color: yellow; color: #d3163b;"> result in a "false positive match"</span>   3. <span style="background-color: yellow; color: #d3163b;">Semantic sensitivity; documents with </span>similar context<span style="background-color: yellow; color: #d3163b;"> but<span style="background-color: yellow; color: #d3163b;"> different </span>term vocabulary won't be associated, resulting in a "false negative match".</span>   4. <span style="background-color: yellow; color: #d3163b;">The order in which</span> the<span style="background-color: yellow; color: #d3163b;"> terms appear in the document is lost in </span>the<span style="background-color: yellow; color: #d3163b;"> vector space representation.</span></pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre>A <span style="background-color: yellow; color: #d3163b;">Vector space model (or term vector model) is an algebraic </span>way of<span style="background-color: yellow; color: #d3163b;"> representing text documents (and any objects, in general) as vectors of identifiers, such as</span><span style="background-color: yellow; color: #d3163b;"> index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings. Its first </span>application<span style="background-color: yellow; color: #d3163b;"> was in the SMART Information Retrieval System.A document </span>can be<span style="background-color: yellow; color: #d3163b;"> <span style="background-color: yellow; color: #d3163b;">represented</span> as a vector. E</span>very<span style="background-color: yellow; color: #d3163b;"> dimension </span>relate<span style="background-color: yellow; color: #d3163b;">s to a </span>different<span style="background-color: yellow; color: #d3163b;"> term. If a term </span>appea<span style="background-color: yellow; color: #d3163b;">rs in the document, </span>the term<span style="background-color: yellow; color: #d3163b;">s value in the vector is non-zero. </span>Many<span style="background-color: yellow; color: #d3163b;"> different </span>method<span style="background-color: yellow; color: #d3163b;">s of c</span>alcula<span style="background-color: yellow; color: #d3163b;">ting these values, </span>sometimes<span style="background-color: yellow; color: #d3163b;"> known as (term) weights, have been developed. </span><span style="background-color: yellow; color: #d3163b;">tf-idf weighting </span>is o<span style="background-color: yellow; color: #d3163b;">ne of the </span>most well<span style="background-color: yellow; color: #d3163b;"> known schemes</span>.<span style="background-color: yellow; color: #d3163b;"> (see </span>below example<span style="background-color: yellow; color: #d3163b;">).The definition of</span> a<span style="background-color: yellow; color: #d3163b;"> term depends on the application. </span>Normally a term is a<span style="background-color: yellow; color: #d3163b;"> single word</span><span style="background-color: yellow; color: #d3163b;">, keyword</span>, or a longer phrase<span style="background-color: yellow; color: #d3163b;">. If the words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the corpus).The vector space model has </span>some<span style="background-color: yellow; color: #d3163b;"> limitations:</span>1.Longer<span style="background-color: yellow; color: #d3163b;"> documents are </span><span style="background-color: yellow; color: #d3163b;">represented</span> poorly<span style="background-color: yellow; color: #d3163b;"> because the</span> documents<span style="background-color: yellow; color: #d3163b;"> have poor similarity values (</span>namely <span style="background-color: yellow; color: #d3163b;">a small scalar product and a large dimensionality)</span>2.<span style="background-color: yellow; color: #d3163b;">Search keywords </span>have to<span style="background-color: yellow; color: #d3163b;"> precisely match document terms; word substrings </span>could potentially<span style="background-color: yellow; color: #d3163b;"> result in a "false positive match"</span>3.<span style="background-color: yellow; color: #d3163b;">Semantic sensitivity; documents with </span>a<span style="background-color: yellow; color: #d3163b;"> similar context</span>,<span style="background-color: yellow; color: #d3163b;"> but<span style="background-color: yellow; color: #d3163b;"> different </span>term vocabulary won't be associated, resulting in a "false negative match".</span>4.<span style="background-color: yellow; color: #d3163b;">The order in which</span><span style="background-color: yellow; color: #d3163b;"> terms appear in the document is lost in </span>a<span style="background-color: yellow; color: #d3163b;"> vector space representation.</span></pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>8. g0pE_taske.txt vs g3pC_taske.txt</description>
            <h2>Similarity: 83.85%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">dynamic <span style="background-color: yellow; color: #d3163b;">program</span>ming</span><span style="background-color: yellow; color: #d3163b;"> is a method of </span><span style="background-color: yellow; color: #d3163b;">solving </span><span style="background-color: yellow; color: #d3163b;">problem</span>s<span style="background-color: yellow; color: #d3163b;"> that </span>exhibit<span style="background-color: yellow; color: #d3163b;"> the properties</span> <span style="background-color: yellow; color: #d3163b;">of overlapping sub<span style="background-color: yellow; color: #d3163b;">problem</span>s and optimal substructure</span> (described below). T<span style="background-color: yellow; color: #d3163b;">he method takes much less time than </span><span style="background-color: yellow; color: #d3163b;">naive methods.</span> T<span style="background-color: yellow; color: #d3163b;">he word "<span style="background-color: yellow; color: #d3163b;">program</span>ming" </span>i<span style="background-color: yellow; color: #d3163b;">n "<span style="background-color: yellow; color: #d3163b;">dynamic <span style="background-color: yellow; color: #d3163b;">program</span>ming</span>"</span> <span style="background-color: yellow; color: #d3163b;">has no</span> particular<span style="background-color: yellow; color: #d3163b;"> connection to computer <span style="background-color: yellow; color: #d3163b;">program</span>ming at all, </span>and instead <span style="background-color: yellow; color: #d3163b;">comes from the term "mathematical <span style="background-color: yellow; color: #d3163b;">program</span>ming",</span> <span style="background-color: yellow; color: #d3163b;">a synonym for optimi</span>z<span style="background-color: yellow; color: #d3163b;">ation. Thus, the "<span style="background-color: yellow; color: #d3163b;">program</span>" is the optimal</span> plan for<span style="background-color: yellow; color: #d3163b;"> action<span style="background-color: yellow; color: #d3163b;"> that </span>is </span><span style="background-color: yellow; color: #d3163b;">produced. For </span>instance, a finalized <span style="background-color: yellow; color: #d3163b;">schedule of events at an exhibition is sometimes called a</span> <span style="background-color: yellow; color: #d3163b;">program</span><span style="background-color: yellow; color: #d3163b;">. Programming, in this sense, means finding an</span> <span style="background-color: yellow; color: #d3163b;">acceptable pla</span>n of actio<span style="background-color: yellow; color: #d3163b;">n, an algorithm.</span></pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre>In computer science and mathematics, <span style="background-color: yellow; color: #d3163b;">dynamic <span style="background-color: yellow; color: #d3163b;">program</span>ming</span>is a method of <span style="background-color: yellow; color: #d3163b;">problem</span> <span style="background-color: yellow; color: #d3163b;">solving </span>that utilises<span style="background-color: yellow; color: #d3163b;"> the properties</span><span style="background-color: yellow; color: #d3163b;">of overlapping sub<span style="background-color: yellow; color: #d3163b;">problem</span>s and optimal substructure</span>. And thust<span style="background-color: yellow; color: #d3163b;">he method takes much less time than </span>more <span style="background-color: yellow; color: #d3163b;">naive methods.</span>I<span style="background-color: yellow; color: #d3163b;">n "<span style="background-color: yellow; color: #d3163b;">dynamic <span style="background-color: yellow; color: #d3163b;">program</span>ming</span>"</span>, t<span style="background-color: yellow; color: #d3163b;">he word "<span style="background-color: yellow; color: #d3163b;">program</span>ming" </span><span style="background-color: yellow; color: #d3163b;">has no</span>real<span style="background-color: yellow; color: #d3163b;"> connection to computer <span style="background-color: yellow; color: #d3163b;">program</span>ming at all, </span>it actually<span style="background-color: yellow; color: #d3163b;">comes from the term "mathematical <span style="background-color: yellow; color: #d3163b;">program</span>ming",</span><span style="background-color: yellow; color: #d3163b;">a synonym for optimi</span>s<span style="background-color: yellow; color: #d3163b;">ation. Thus, the "<span style="background-color: yellow; color: #d3163b;">program</span>" is the optimal</span>plan of<span style="background-color: yellow; color: #d3163b;"> action<span style="background-color: yellow; color: #d3163b;"> that </span>is </span>being <span style="background-color: yellow; color: #d3163b;">produced. For </span>example, a<span style="background-color: yellow; color: #d3163b;">schedule of events at an exhibition is sometimes called a</span><span style="background-color: yellow; color: #d3163b;">program</span>me<span style="background-color: yellow; color: #d3163b;">. Programming, in this sense, means finding an</span><span style="background-color: yellow; color: #d3163b;">acceptable pla</span><span style="background-color: yellow; color: #d3163b;">n, an algorithm.</span></pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>9. g2pB_taskd.txt vs g4pC_taskd.txt</description>
            <h2>Similarity: 80.22%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">In<span style="background-color: yellow; color: #d3163b;"> probability </span>theory,<span style="background-color: yellow; color: #d3163b;"> Bayes' </span>theorem </span>also called<span style="background-color: yellow; color: #d3163b;"> Bayes' </span>law after Rev Thomas Bayes compar<span style="background-color: yellow; color: #d3163b;">es the conditional and marginal probabilities of two random events. It is </span>often<span style="background-color: yellow; color: #d3163b;"> used to c</span>alcula<span style="background-color: yellow; color: #d3163b;">te posterior probabilities given observations. For </span>exampl<span style="background-color: yellow; color: #d3163b;">e, a patient may be observed to have certain symptoms.<span style="background-color: yellow; color: #d3163b;"> Bayes' </span>theorem can be<span style="background-color: yellow; color: #d3163b;"> used to c</span></span>alcula<span style="background-color: yellow; color: #d3163b;">te the </span>likelihood<span style="background-color: yellow; color: #d3163b;"> that a proposed </span>analy<span style="background-color: yellow; color: #d3163b;">sis is </span>accurate<span style="background-color: yellow; color: #d3163b;">, given that observation. As a</span>n offici<span style="background-color: yellow; color: #d3163b;">al theorem,<span style="background-color: yellow; color: #d3163b;"> Bayes' </span>theorem is valid in all </span>universal<span style="background-color: yellow; color: #d3163b;"> interpretations of probability. However, it plays a </span>fundament<span style="background-color: yellow; color: #d3163b;">al role in the debate around the foundations of statistics: frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. </span><span style="background-color: yellow; color: #d3163b;">Frequentists assign probabilities to random events according to their frequencies of </span>happening<span style="background-color: yellow; color: #d3163b;"> or to subsets of populations as proportions of the whole. </span>Whilst Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. <span style="background-color: yellow; color: #d3163b;">The articles on Bayesian<span style="background-color: yellow; color: #d3163b;"> probability </span>and frequentist<span style="background-color: yellow; color: #d3163b;"> probability </span>discuss these debates in greater detail.</span>Bayes' theorem compar<span style="background-color: yellow; color: #d3163b;">es the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability</span>.<span style="background-color: yellow; color: #d3163b;">Each term in<span style="background-color: yellow; color: #d3163b;"> Bayes' </span>theorem has a conventional name:</span><span style="background-color: yellow; color: #d3163b;">P(A) is the pr</span>evious<span style="background-color: yellow; color: #d3163b;"> probability </span><span style="background-color: yellow; color: #d3163b;"> of A. It is "pr</span>evious<span style="background-color: yellow; color: #d3163b;">" in the sense that it does not take into account any information about B.</span><span style="background-color: yellow; color: #d3163b;">P(A|B) is the conditional<span style="background-color: yellow; color: #d3163b;"> probability </span>of A, given B. It is also called the </span>subsequent<span style="background-color: yellow; color: #d3163b;"><span style="background-color: yellow; color: #d3163b;"> probability </span>because it is derived from or depends upon the specified value of B.</span><span style="background-color: yellow; color: #d3163b;">P(B|A) is the conditional<span style="background-color: yellow; color: #d3163b;"> probability </span>of B given A.</span><span style="background-color: yellow; color: #d3163b;">P(B) is the pr</span>evious.</pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">In<span style="background-color: yellow; color: #d3163b;"> probability </span>theory,<span style="background-color: yellow; color: #d3163b;"> Bayes' </span>theorem </span>relat<span style="background-color: yellow; color: #d3163b;">es the conditional and marginal probabilities of two random events. It is </span>usually be<span style="background-color: yellow; color: #d3163b;"> used to c</span>ompu<span style="background-color: yellow; color: #d3163b;">te posterior probabilities given observations. For </span>instanc<span style="background-color: yellow; color: #d3163b;">e, a patient may be observed to have certain symptoms.<span style="background-color: yellow; color: #d3163b;"> Bayes' </span>theorem can be<span style="background-color: yellow; color: #d3163b;"> used to c</span></span>ompu<span style="background-color: yellow; color: #d3163b;">te the </span>probability<span style="background-color: yellow; color: #d3163b;"> that a proposed </span>diagno<span style="background-color: yellow; color: #d3163b;">sis is </span>correct<span style="background-color: yellow; color: #d3163b;">, given that observation. As a</span> form<span style="background-color: yellow; color: #d3163b;">al theorem,<span style="background-color: yellow; color: #d3163b;"> Bayes' </span>theorem is valid in all </span>common<span style="background-color: yellow; color: #d3163b;"> interpretations of probability. However, it plays a </span>centr<span style="background-color: yellow; color: #d3163b;">al role in the debate around the foundations of statistics: frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. </span><span style="background-color: yellow; color: #d3163b;">The articles on Bayesian<span style="background-color: yellow; color: #d3163b;"> probability </span>and frequentist<span style="background-color: yellow; color: #d3163b;"> probability </span>discuss these debates in greater detail.</span> <span style="background-color: yellow; color: #d3163b;">Frequentists assign probabilities to random events according to their frequencies of </span>occurrence<span style="background-color: yellow; color: #d3163b;"> or to subsets of populations as proportions of the whole. </span>At the same time, <span style="background-color: yellow; color: #d3163b;"> Bayesians describe probabilities in terms of beliefs and degrees of uncertainty.<span style="background-color: yellow; color: #d3163b;"> Bayes' </span>theorem </span>relat<span style="background-color: yellow; color: #d3163b;">es the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability</span>:<span style="background-color: yellow; color: #d3163b;">Each term in<span style="background-color: yellow; color: #d3163b;"> Bayes' </span>theorem has a conventional name:</span>•<span style="background-color: yellow; color: #d3163b;">P(A) is the pr</span>ior<span style="background-color: yellow; color: #d3163b;"> probability </span>or marginal probability<span style="background-color: yellow; color: #d3163b;"> of A. It is "pr</span>ior<span style="background-color: yellow; color: #d3163b;">" in the sense that it does not take into account any information about B.</span> •<span style="background-color: yellow; color: #d3163b;">P(A|B) is the conditional<span style="background-color: yellow; color: #d3163b;"> probability </span>of A, given B. It is also called the </span>posterior<span style="background-color: yellow; color: #d3163b;"><span style="background-color: yellow; color: #d3163b;"> probability </span>because it is derived from or depends upon the specified value of B.</span> •<span style="background-color: yellow; color: #d3163b;">P(B|A) is the conditional<span style="background-color: yellow; color: #d3163b;"> probability </span>of B given A.</span> •<span style="background-color: yellow; color: #d3163b;">P(B) is the pr</span>ior or marginal<span style="background-color: yellow; color: #d3163b;"> probability </span>of B, and acts as a normalizing constant. Intuitively,<span style="background-color: yellow; color: #d3163b;"> Bayes' </span>theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.</pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>10. g2pB_taskd.txt vs g3pA_taskd.txt</description>
            <h2>Similarity: 79.81%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">In<span style="background-color: yellow; color: #d3163b;"> probability </span>theory, Bayes' theorem </span>also<span style="background-color: yellow; color: #d3163b;"> called Bayes' law after Rev Thomas Bayes</span> compar<span style="background-color: yellow; color: #d3163b;">es the conditional and marginal probabilities of two random events. It is often used to c</span>alcula<span style="background-color: yellow; color: #d3163b;">te posterior probabilities given observations</span>. F<span style="background-color: yellow; color: #d3163b;">or example, a patient may be observed to have certain symptoms</span><span style="background-color: yellow; color: #d3163b;">. Bayes' theorem can be used to c</span>alcula<span style="background-color: yellow; color: #d3163b;">te the </span>likelihood<span style="background-color: yellow; color: #d3163b;"> that a proposed </span>analy<span style="background-color: yellow; color: #d3163b;">sis is </span>accurate<span style="background-color: yellow; color: #d3163b;">, given that observation.</span> As an offici<span style="background-color: yellow; color: #d3163b;">al theorem, Bayes' theorem is valid in all </span>universal<span style="background-color: yellow; color: #d3163b;"> interpretations of probability. However, it plays a </span>fundament<span style="background-color: yellow; color: #d3163b;">al role in the debate around the foundations of statistics</span>:<span style="background-color: yellow; color: #d3163b;"> frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. Frequentists assign probabilities to random events according to their frequencies of </span>happening<span style="background-color: yellow; color: #d3163b;"> or to subsets of populations as proportions of the whole</span>. Whilst<span style="background-color: yellow; color: #d3163b;"> Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. The articles on Bayesian<span style="background-color: yellow; color: #d3163b;"> probability </span>and frequentist<span style="background-color: yellow; color: #d3163b;"> probability </span>discuss these debates in greater detail.Bayes' theorem </span>compar<span style="background-color: yellow; color: #d3163b;">es the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability</span><span style="background-color: yellow; color: #d3163b;">.Each term in Bayes' theorem has a conventional name:P(A) is the pr</span>evious<span style="background-color: yellow; color: #d3163b;"> probability </span><span style="background-color: yellow; color: #d3163b;"> of A. It is "pr</span>evious<span style="background-color: yellow; color: #d3163b;">" in the sense that it does not take into account any information about B.</span><span style="background-color: yellow; color: #d3163b;">P(A|B) is the conditional<span style="background-color: yellow; color: #d3163b;"> probability </span>of A, given B. It is also called the </span>subsequent<span style="background-color: yellow; color: #d3163b;"><span style="background-color: yellow; color: #d3163b;"> probability </span>because it is derived from or depends upon the specified value of B.P(B|A) is the conditional<span style="background-color: yellow; color: #d3163b;"> probability </span>of B given A.P(B) is the pr</span>evious.</pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre><span style="background-color: yellow; color: #d3163b;">In<span style="background-color: yellow; color: #d3163b;"> probability </span>theory, Bayes' theorem </span>(often<span style="background-color: yellow; color: #d3163b;"> called Bayes' law after Rev Thomas Bayes</span>) relat<span style="background-color: yellow; color: #d3163b;">es the conditional and marginal probabilities of two random events. It is often used to c</span>ompu<span style="background-color: yellow; color: #d3163b;">te posterior probabilities given observations</span> (f<span style="background-color: yellow; color: #d3163b;">or example, a patient may be observed to have certain symptoms</span>)<span style="background-color: yellow; color: #d3163b;">. Bayes' theorem can be used to c</span>ompu<span style="background-color: yellow; color: #d3163b;">te the </span>probability<span style="background-color: yellow; color: #d3163b;"> that a proposed </span>diagno<span style="background-color: yellow; color: #d3163b;">sis is </span>correct<span style="background-color: yellow; color: #d3163b;">, given that observation.</span>As a form<span style="background-color: yellow; color: #d3163b;">al theorem, Bayes' theorem is valid in all </span>common<span style="background-color: yellow; color: #d3163b;"> interpretations of probability. However, it plays a </span>centr<span style="background-color: yellow; color: #d3163b;">al role in the debate around the foundations of statistics</span>;<span style="background-color: yellow; color: #d3163b;"> frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. Frequentists assign probabilities to random events according to their frequencies of </span>occurrence<span style="background-color: yellow; color: #d3163b;"> or to subsets of populations as proportions of the whole</span>, while<span style="background-color: yellow; color: #d3163b;"> Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. The articles on Bayesian<span style="background-color: yellow; color: #d3163b;"> probability </span>and frequentist<span style="background-color: yellow; color: #d3163b;"> probability </span>discuss these debates in greater detail.Bayes' theorem </span>relat<span style="background-color: yellow; color: #d3163b;">es the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability</span>:    P(A|B) = (P(B | A) x P(A)) / P(B)<span style="background-color: yellow; color: #d3163b;">.Each term in Bayes' theorem has a conventional name:P(A) is the pr</span>ior<span style="background-color: yellow; color: #d3163b;"> probability </span>or marginal probability<span style="background-color: yellow; color: #d3163b;"> of A. It is "pr</span>ior<span style="background-color: yellow; color: #d3163b;">" in the sense that it does not take into account any information about B.</span> <span style="background-color: yellow; color: #d3163b;">P(A|B) is the conditional<span style="background-color: yellow; color: #d3163b;"> probability </span>of A, given B. It is also called the </span>posterior<span style="background-color: yellow; color: #d3163b;"><span style="background-color: yellow; color: #d3163b;"> probability </span>because it is derived from or depends upon the specified value of B.P(B|A) is the conditional<span style="background-color: yellow; color: #d3163b;"> probability </span>of B given A.P(B) is the pr</span>ior or marginal<span style="background-color: yellow; color: #d3163b;"> probability </span>of B, and acts as a normalizing constant.Intuitively, Bayes' theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.</pre></pre>
                </div>
            </div>
        </div></body></html>