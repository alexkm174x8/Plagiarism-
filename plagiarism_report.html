<html>
<head>
<title>Plagiarism Detector</title>
<style>
@font-face {
  font-family: Nohemi;
  src: url("Nohemi-Medium.otf")
}
@font-face {
  font-family: OffBit;
  src: url("OffBit.ttf")
}
body {
  background-color: #ffa1d7;
  font-family: Nohemi;
  color: #d3163b;
  margin: 0;
  padding: 0;
  box-sizing: border-box;
}
.container {
  max-width: 1370px;
  margin: 20px auto;
  padding: 20px;
  border: 1px solid #d3163b;
  background-color: #fdf2f5;
  box-sizing: border-box;
}
pre {
  border: 1px solid #d3163b;
  padding: 10px;
  background-color: #f8f7f8;
  width: 100%;
  white-space: pre-wrap;
  word-wrap: break-word;
  box-sizing: border-box;
}
h1 {
  font-size: 60px;
  text-align: center;
  margin-top: 50px;
  font-family: OffBit;
}
h2 {
  margin-top: 5px;
  margin-bottom: 5px;
  font-size: 20px;
}
h3 {
  margin-top: 0px;
  font-size: 22px;
  margin-bottom: 45px;
  margin-left: 60px;
  margin-right: 60px;
}
description {
  margin-top: 0px;
  font-size: 25px;
  margin-bottom: 45px;
  font-family: OffBit;
}
.similarity-box {
  margin-bottom: 20px;
}
.text-container {
  display: block;
}
.highlight {
  background-color: yellow;
  color: #d3163b;
}
</style>
</head>
<body>
<h1>Plagiarism Detector •*.✸ </h1>
<h3 style="text-align: center;">Web App in Golang that compares the content of different texts about the same topic, calculating and highlighting their similarity using a Suffix Array that retrieves the Longest Common Substring (LCS).</h3>
        <div class="container">
            <description>1. orig_taskd.txt vs g3pA_taskd.txt</description>
            <h2>Similarity: 60.74%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre>In probability theory, Bayes' theorem (often called Bayes' law after Rev Thomas Bayes) relates the conditional and marginal probabilities of two random events. It is often used to compute posterior probabilities given observations. For example, a patient may be observed to have certain symptoms. Bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation. (See example 2)
As a formal theorem, Bayes' theorem is valid in all common interpretations of probability. However, it plays a central role in the debate around the foundations of statistics:<span style="background-color: yellow; color: #d3163b;"> frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. Frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole, while Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. The articles on Bayesian probability and frequentist probability discuss these debates in greater detail.
</span>Bayes' theorem relates the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability:
    P(A|B) = \frac{P(B | A)\, P(A)}{P(B)}.
Each term in Bayes' theorem has a conventional name:
    * P(A) is the prior probability or marginal probability of A. It is "prior" in the sense that it does not take into account any information about B.
    * P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from or depends upon the specified value of B.
    * P(B|A) is the conditional probability of B given A.
    * P(B) is the prior or marginal probability of B, and acts as a normalizing constant.
Intuitively, Bayes' theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.
</pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre>In probability theory, Bayes' theorem (often called Bayes' law after Rev Thomas Bayes) relates the conditional and marginal probabilities of two random events. It is often used to compute posterior probabilities given observations (for example, a patient may be observed to have certain symptoms). Bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation.

As a formal theorem, Bayes' theorem is valid in all common interpretations of probability. However, it plays a central role in the debate around the foundations of statistics;<span style="background-color: yellow; color: #d3163b;"> frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. Frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole, while Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. The articles on Bayesian probability and frequentist probability discuss these debates in greater detail.
</span>
Bayes' theorem relates the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability:

    P(A|B) = (P(B | A) x P(A)) / P(B).

Each term in Bayes' theorem has a conventional name:
P(A) is the prior probability or marginal probability of A. It is "prior" in the sense that it does not take into account any information about B.
 P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from or depends upon the specified value of B.
P(B|A) is the conditional probability of B given A.
P(B) is the prior or marginal probability of B, and acts as a normalizing constant.

Intuitively, Bayes' theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.</pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>2. orig_taska.txt vs g0pE_taska.txt</description>
            <h2>Similarity: 58.81%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre>In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula.

The new classes, known as derived classes, take over (or inherit) attributes and behavior of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification.

Inheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization (what is known about specific entities is applied to a wider group given a belongs relation can be established) and cognitive economy (less information needs to be stored about each specific entity, only its particularities).

<span style="background-color: yellow; color: #d3163b;">Inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant.</span>

An advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code.
Inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.

Complex inheritance, or inheritance used within a design that is not sufficiently mature, may lead to the Yo-yo problem.</pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre>In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula. The new classes, known as derived classes, take over (or inherit) attribute and behaviour of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification. Inheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization (what is known about specific entities is applied to a wider group given a belongs relation can be established) and cognitive economy (less information needs to be stored about each specific entity, only its particularities). <span style="background-color: yellow; color: #d3163b;">Inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant.</span> An advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code. Inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor. 
</pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>3. orig_taska.txt vs g4pC_taska.txt</description>
            <h2>Similarity: 48.92%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre>In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula.

The new classes, known as derived classes, take over (or inherit) attributes and behavior of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification.

Inheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization (what is known about specific entities is applied to a wider group given a belongs relation can be established) and cognitive economy (less information needs to be stored about each specific entity, only its particularities).
<span style="background-color: yellow; color: #d3163b;">
Inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant.
</span>
An advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code.
Inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.

Complex inheritance, or inheritance used within a design that is not sufficiently mature, may lead to the Yo-yo problem.</pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre>In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula
Inheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization and cognitive economy (less information needs to be stored about each specific entity, only its particularities).
The new classes, known as derived classes, take over (or inherit) attributes and behavior of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification.<span style="background-color: yellow; color: #d3163b;">
Inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant.
</span>An advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code.
Inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.
Complex inheritance, or inheritance used within a design that is not sufficiently mature, may lead to the Yo-yo problem.
</pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>4. g4pC_taskd.txt vs orig_taskd.txt</description>
            <h2>Similarity: 48.04%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre>In probability theory, Bayes' theorem relates the conditional and marginal probabilities of two random events. It is usually be used to compute posterior probabilities given observations. For instance, a patient may be observed to have certain symptoms. Bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation. <span style="background-color: yellow; color: #d3163b;">
As a formal theorem, Bayes' theorem is valid in all common interpretations of probability. However, it plays a central role in the debate around the foundations of statistics: frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. </span>The articles on Bayesian probability and frequentist probability discuss these debates in greater detail. Frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole. At the same time,  Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. 
Bayes' theorem relates the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability:
Each term in Bayes' theorem has a conventional name:
•	P(A) is the prior probability or marginal probability of A. It is "prior" in the sense that it does not take into account any information about B. 
•	P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from or depends upon the specified value of B. 
•	P(B|A) is the conditional probability of B given A. 
•	P(B) is the prior or marginal probability of B, and acts as a normalizing constant. 
Intuitively, Bayes' theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.
</pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre>In probability theory, Bayes' theorem (often called Bayes' law after Rev Thomas Bayes) relates the conditional and marginal probabilities of two random events. It is often used to compute posterior probabilities given observations. For example, a patient may be observed to have certain symptoms. Bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation. (See example 2)<span style="background-color: yellow; color: #d3163b;">
As a formal theorem, Bayes' theorem is valid in all common interpretations of probability. However, it plays a central role in the debate around the foundations of statistics: frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. </span>Frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole, while Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. The articles on Bayesian probability and frequentist probability discuss these debates in greater detail.
Bayes' theorem relates the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability:
    P(A|B) = \frac{P(B | A)\, P(A)}{P(B)}.
Each term in Bayes' theorem has a conventional name:
    * P(A) is the prior probability or marginal probability of A. It is "prior" in the sense that it does not take into account any information about B.
    * P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from or depends upon the specified value of B.
    * P(B|A) is the conditional probability of B given A.
    * P(B) is the prior or marginal probability of B, and acts as a normalizing constant.
Intuitively, Bayes' theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.
</pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>5. orig_taskc.txt vs g2pA_taskc.txt</description>
            <h2>Similarity: 47.86%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre>Vector space model (or term vector model) is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers, such as, for example, index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings. Its first use was in the SMART Information Retrieval System.
A document is represented as a vector. Each dimension corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is tf-idf weighting (see the example below).
The definition of term depends on the application. Typically terms are single words, keywords, or longer phrases<span style="background-color: yellow; color: #d3163b;">. If the words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the corpus).
The vector space model has </span>the following limitations:
   1. Long documents are poorly represented because they have poor similarity values (a small scalar product and a large dimensionality)
   2. Search keywords must precisely match document terms; word substrings might result in a "false positive match"
   3. Semantic sensitivity; documents with similar context but different term vocabulary won't be associated, resulting in a "false negative match".
   4. The order in which the terms appear in the document is lost in the vector space representation.
</pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre>A Vector space model (or term vector model) is an algebraic way of representing text documents (and any objects, in general) as vectors of identifiers, such as index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings. Its first application was in the SMART Information Retrieval System.
A document can be represented as a vector. Every dimension relates to a different term. If a term appears in the document, the terms value in the vector is non-zero. Many different methods of calculating these values, sometimes known as (term) weights, have been developed. tf-idf weighting is one of the most well known schemes. (see below example).
The definition of a term depends on the application. Normally a term is a single word, keyword, or a longer phrase<span style="background-color: yellow; color: #d3163b;">. If the words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary (the number of distinct words occurring in the corpus).
The vector space model has </span>some limitations:
1.	Longer documents are represented poorly because the documents have poor similarity values (namely a small scalar product and a large dimensionality)
2.	Search keywords have to precisely match document terms; word substrings could potentially result in a "false positive match"
3.	Semantic sensitivity; documents with a similar context, but different term vocabulary won't be associated, resulting in a "false negative match".
4.	The order in which terms appear in the document is lost in a vector space representation.
</pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>6. g4pC_taska.txt vs g0pE_taska.txt</description>
            <h2>Similarity: 45.64%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre>In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula
Inheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization and cognitive economy (less information needs to be stored about each specific entity, only its particularities).
The new classes, known as derived classes, take over (or inherit) attributes and behavior of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification.
<span style="background-color: yellow; color: #d3163b;">Inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant.</span>
An advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code.
Inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.
Complex inheritance, or inheritance used within a design that is not sufficiently mature, may lead to the Yo-yo problem.
</pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre>In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula. The new classes, known as derived classes, take over (or inherit) attribute and behaviour of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification. Inheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization (what is known about specific entities is applied to a wider group given a belongs relation can be established) and cognitive economy (less information needs to be stored about each specific entity, only its particularities). <span style="background-color: yellow; color: #d3163b;">Inheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a "fruit" is a generalization of "apple", "orange", "mango" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant.</span> An advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code. Inheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor. 
</pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>7. g4pC_taskd.txt vs g3pA_taskd.txt</description>
            <h2>Similarity: 45.22%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre>In probability theory, Bayes' theorem relates the conditional and marginal probabilities of two random events. It is usually be used to compute posterior probabilities given observations. For instance, a patient may be observed to have certain symptoms. Bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation. <span style="background-color: yellow; color: #d3163b;">
As a formal theorem, Bayes' theorem is valid in all common interpretations of probability. However, it plays a central role in the debate around the foundations of statistics</span>: frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. The articles on Bayesian probability and frequentist probability discuss these debates in greater detail. Frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole. At the same time,  Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. 
Bayes' theorem relates the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability:
Each term in Bayes' theorem has a conventional name:
•	P(A) is the prior probability or marginal probability of A. It is "prior" in the sense that it does not take into account any information about B. 
•	P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from or depends upon the specified value of B. 
•	P(B|A) is the conditional probability of B given A. 
•	P(B) is the prior or marginal probability of B, and acts as a normalizing constant. 
Intuitively, Bayes' theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.
</pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre>In probability theory, Bayes' theorem (often called Bayes' law after Rev Thomas Bayes) relates the conditional and marginal probabilities of two random events. It is often used to compute posterior probabilities given observations (for example, a patient may be observed to have certain symptoms). Bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation.
<span style="background-color: yellow; color: #d3163b;">
As a formal theorem, Bayes' theorem is valid in all common interpretations of probability. However, it plays a central role in the debate around the foundations of statistics</span>; frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. Frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole, while Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. The articles on Bayesian probability and frequentist probability discuss these debates in greater detail.

Bayes' theorem relates the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability:

    P(A|B) = (P(B | A) x P(A)) / P(B).

Each term in Bayes' theorem has a conventional name:
P(A) is the prior probability or marginal probability of A. It is "prior" in the sense that it does not take into account any information about B.
 P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from or depends upon the specified value of B.
P(B|A) is the conditional probability of B given A.
P(B) is the prior or marginal probability of B, and acts as a normalizing constant.

Intuitively, Bayes' theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.</pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>8. g2pB_taskd.txt vs g3pA_taskd.txt</description>
            <h2>Similarity: 44.98%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre>
In probability theory, Bayes' theorem also called Bayes' law after Rev Thomas Bayes compares the conditional and marginal probabilities of two random events. It is often used to calculate posterior probabilities given observations. For example, a patient may be observed to have certain symptoms. Bayes' theorem can be used to calculate the likelihood that a proposed analysis is accurate, given that observation. 
As an official theorem, Bayes' theorem is valid in all universal interpretations of probability. However, it plays a fundamental role in the debate around the foundations of statistics: frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications.
 Frequentists assign probabilities to random events according to their frequencies of happening or to subsets of populations as proportions of the whole. Whilst<span style="background-color: yellow; color: #d3163b;"> Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. The articles on Bayesian probability and frequentist probability discuss these debates in greater detail.
</span>Bayes' theorem compares the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability.
Each term in Bayes' theorem has a conventional name:
P(A) is the previous probability  of A. It is "previous" in the sense that it does not take into account any information about B.
P(A|B) is the conditional probability of A, given B. It is also called the subsequent probability because it is derived from or depends upon the specified value of B.
P(B|A) is the conditional probability of B given A.
P(B) is the previous.
</pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre>In probability theory, Bayes' theorem (often called Bayes' law after Rev Thomas Bayes) relates the conditional and marginal probabilities of two random events. It is often used to compute posterior probabilities given observations (for example, a patient may be observed to have certain symptoms). Bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation.

As a formal theorem, Bayes' theorem is valid in all common interpretations of probability. However, it plays a central role in the debate around the foundations of statistics; frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. Frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole, while<span style="background-color: yellow; color: #d3163b;"> Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. The articles on Bayesian probability and frequentist probability discuss these debates in greater detail.
</span>
Bayes' theorem relates the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability:

    P(A|B) = (P(B | A) x P(A)) / P(B).

Each term in Bayes' theorem has a conventional name:
P(A) is the prior probability or marginal probability of A. It is "prior" in the sense that it does not take into account any information about B.
 P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from or depends upon the specified value of B.
P(B|A) is the conditional probability of B given A.
P(B) is the prior or marginal probability of B, and acts as a normalizing constant.

Intuitively, Bayes' theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.</pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>9. g2pB_taskd.txt vs orig_taskd.txt</description>
            <h2>Similarity: 44.66%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre>
In probability theory, Bayes' theorem also called Bayes' law after Rev Thomas Bayes compares the conditional and marginal probabilities of two random events. It is often used to calculate posterior probabilities given observations. For example, a patient may be observed to have certain symptoms. Bayes' theorem can be used to calculate the likelihood that a proposed analysis is accurate, given that observation. 
As an official theorem, Bayes' theorem is valid in all universal interpretations of probability. However, it plays a fundamental role in the debate around the foundations of statistics: frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications.
 Frequentists assign probabilities to random events according to their frequencies of happening or to subsets of populations as proportions of the whole. Whilst<span style="background-color: yellow; color: #d3163b;"> Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. The articles on Bayesian probability and frequentist probability discuss these debates in greater detail.
Bayes' theorem </span>compares the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability.
Each term in Bayes' theorem has a conventional name:
P(A) is the previous probability  of A. It is "previous" in the sense that it does not take into account any information about B.
P(A|B) is the conditional probability of A, given B. It is also called the subsequent probability because it is derived from or depends upon the specified value of B.
P(B|A) is the conditional probability of B given A.
P(B) is the previous.
</pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre>In probability theory, Bayes' theorem (often called Bayes' law after Rev Thomas Bayes) relates the conditional and marginal probabilities of two random events. It is often used to compute posterior probabilities given observations. For example, a patient may be observed to have certain symptoms. Bayes' theorem can be used to compute the probability that a proposed diagnosis is correct, given that observation. (See example 2)
As a formal theorem, Bayes' theorem is valid in all common interpretations of probability. However, it plays a central role in the debate around the foundations of statistics: frequentist and Bayesian interpretations disagree about the ways in which probabilities should be assigned in applications. Frequentists assign probabilities to random events according to their frequencies of occurrence or to subsets of populations as proportions of the whole, while<span style="background-color: yellow; color: #d3163b;"> Bayesians describe probabilities in terms of beliefs and degrees of uncertainty. The articles on Bayesian probability and frequentist probability discuss these debates in greater detail.
Bayes' theorem </span>relates the conditional and marginal probabilities of events A and B, where B has a non-vanishing probability:
    P(A|B) = \frac{P(B | A)\, P(A)}{P(B)}.
Each term in Bayes' theorem has a conventional name:
    * P(A) is the prior probability or marginal probability of A. It is "prior" in the sense that it does not take into account any information about B.
    * P(A|B) is the conditional probability of A, given B. It is also called the posterior probability because it is derived from or depends upon the specified value of B.
    * P(B|A) is the conditional probability of B given A.
    * P(B) is the prior or marginal probability of B, and acts as a normalizing constant.
Intuitively, Bayes' theorem in this form describes the way in which one's beliefs about observing 'A' are updated by having observed 'B'.
</pre></pre>
                </div>
            </div>
        </div>
        <div class="container">
            <description>10. orig_taskc.txt vs g0pA_taskc.txt</description>
            <h2>Similarity: 43.31%</h2>
            <div class="text-container">
                <div class="text-content">
                    <pre><pre>Vector space model (or term vector model) is an algebraic model for representing text documents (and any objects, in general) as vectors of identifiers, such as, for example, index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings. Its first use was in the SMART Information Retrieval System.
A document is represented as a vector. Each dimension corresponds to a separate term. If a term occurs in the document, its value in the vector is non-zero. Several different ways of computing these values, also known as (term) weights, have been developed. One of the best known schemes is tf-idf weighting (see the example below).
The definition of term depends on the application. Typic<span style="background-color: yellow; color: #d3163b;">ally terms are single words, keywords, or longer phrases. If the words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary</span> (the number of distinct words occurring in the corpus).
The vector space model has the following limitations:
   1. Long documents are poorly represented because they have poor similarity values (a small scalar product and a large dimensionality)
   2. Search keywords must precisely match document terms; word substrings might result in a "false positive match"
   3. Semantic sensitivity; documents with similar context but different term vocabulary won't be associated, resulting in a "false negative match".
   4. The order in which the terms appear in the document is lost in the vector space representation.
</pre></pre>
                </div>
                <div class="text-content">
                    <pre><pre>The vector space model (also called, term vector model) is an algebraic model used to represent text documents, as well as any objects in general, as vectors of identifiers. It is used in information retrieval and was first used in the SMART Information Retrieval System.

A document is represented as a vector and each dimension corresponds to a separate term. If a term appears in the document then its value in the vector is non-zero. Many different ways of calculating these values, also known as (term) weights, have been developed. One of the best known methods is called tf-idf weighting.

The definition of term depends on the application but gener<span style="background-color: yellow; color: #d3163b;">ally terms are single words, keywords, or longer phrases. If the words are chosen to be the terms, the dimensionality of the vector is the number of words in the vocabulary</span>, which is the number of distinct words occurring in the corpus.

The vector space model has several disadvantages. Firstly, long documents are represented badly because they have poor similarity values. Secondly, search keywords must accurately match document terms and substrings of words might result in a "false-positive match". Thirdly, documents with similar context but different term vocabulary will not be associated, resulting in a "false-negative match". Finally, the order in which the terms appear in the document is lost in the vector space representation. 
</pre></pre>
                </div>
            </div>
        </div></body></html>